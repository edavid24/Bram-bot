
I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.

With these words, Alan Turing started his paper called "Computing Machinery and Intelligence", which was published in 1950 in the philosophical journal Mind. It is by many considered the first serious intellectual analysis of the possibility of machine intelligence, and to this day remains a highly influential paper in philosophical discussions about the possibility of machine intelligence. 
Turing’s paper consists of two fairly distinct parts. In the first part, Turing describes what we nowadays know as 'the Turing Test'. In the second part, Turing anticipates various objections to the possibility of machine intelligence, many of which are still commonly given nowadays. In this chapter, we will very much follow the same organization.

The new form of the problem can be described in terms of a game which we call the 'imitation game.”
In his paper, Turing describes the ‘imitation game’, which involves 3 people. A man and a woman are put in a room, while a third person of either gender is put in a separate room. The interrogator is told that there are two people of different gender in the other room, and that the interrogator has to figure out which of the two people, known simply as ‘A’ and ‘B’, is the man, and which is the woman. Now, to make this an interesting game, the man (and let’s suppose the man is ‘A’) is told to answer the questions as if he were a woman, while the woman is simply to answer truthfully. Thus, when asked “Are you a woman”, both A and B will answer “Yes”.
Turing then continues:
We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"

The 'imitation game' that Turing describes here quickly captured the imagination of the public. It was soon called 'Turing's Test', and is now popularly known as 'the Turing Test'. The general idea is that a machine that does so well in the imitation game that the interrogator cannot distinguish (or at least has great trouble doing so) its responses from those given by the human being, is said to 'pass' the test. 
The Turing Test has received mixed reactions. There have been many criticisms of the test, but there have also been defenders of the test. Before we enter any of the arguments for and against the test, though, we should get clear on exactly what it is that Turing is proposing. Or, more to the point: how should we think about the Turing Test?

Some commentators of Turing’s paper will claim that with his Imitation Game, or Turing Test, Turing tried to create a working definition of ‘intelligence’. Indeed, 14 years before this paper, Turing wrote another highly influential paper in which he defined what are now known as Turing machines, which was a mathematical definition proposed to capture the intuitive notion of a 'computer’. Indeed, the machines that Turing talks about in the context of his Imitation Game are computers, rather than just any machine. 
Turing machines will be discussed at length in Chapter 3, but for now it suffices to point out that with his Turing machine, Turing gave a hard and precise delineation for a fuzzy concept in the hopes of obtaining an objective and scientifically useful theory to explain and predict what, and what not, could be figured using any kind of 'effective method' or ‘computation’. In this, Turing succeeded spectacularly. His precise mathematical definition allowed Turing to prove important results in computability theory, and apparently his proposed definition (now known as 'Turing-computation') captured the notion of 'effective method' well enough that these mathematical results seem to describe the actual world we live in with a high degree of accuracy. 
So, maybe with his proposal of the 'imitation game', Turing was similarly hoping to obtain an objective way to classify a machine as belonging to an interesting class of objects, allowing us to make useful scientific explanations and predictions regarding that machine's capabilities related to the kinds of things we would normally associate with 'thinking' and 'intelligence'?
This is certainly an interesting way of looking at the Turing Test. In fact, the situation we are finding ourselves can be compared to that of the case of 'planets'. People can (and probably forever will) argue whether Pluto is a 'planet' or not. However, by making a clear definition of what is a 'planet', we can at least make some objective and scientific headway in terms of making sense of the world we live in. Likewise, we can probably forever debate whether the Turing Test captures the notion of 'intelligence' or 'thinking' or not, but at least it seems to give us a practical and potentially more objective tool to make some important delineations about the objects in the world around us. 
Moreover, by saying that Pluto is a 'dwarf-planet', we have started to make more fine-grained distinctions, ontologies, and taxonomies between the different objects in our solar system, helping us to obtain more precise and useful explanations and predictions. And note how Turing's notion of Turing-computation led to various other forms of computation, such as finite-state machines, and push-down automata, each with their own objective definitions and associated mathematical theorems, all reflective of what we hope are different 'natural kinds' that exist in the world we actually live in. 
Likewise, it is probably fruitful to differentiate between different kinds of intelligence. Indeed, by putting different constraints on how the imitation game is played, we might obtain the potential to be able to differentiate between different kinds of intelligence. Was this what Turing proposed when he laid out the Imitation Game?
As exciting as the prospect sounds, this idea immediately goes against what Turing writes in the starting passage of his paper. Apparently Turing does not think that we should try to answer the question of 'Can machines think' by trying to define such notions as 'thinking' or 'intelligence'. Indeed, he seems pretty explicit in his proposal to address any questions as to the possibility of machine intelligence by considering the imitation game instead. That is, Turing is looking for a way to address the question of machine intelligence that is able to avoid having to define 'intelligence'': something that will provide us some guidance as to the question of whether or not machines can think, even as we don't really know what 'thinking' really is. 
Of course, as such one could hold the view that instead of a theoretical definition, maybe Turing was still trying to provide some kind of operational definition of thinking and intelligence. For example, counting the number of cycles of radiation a cesium-133 atom makes doesn't really tell us what time is, but nevertheless provides us with a measurement as to how much time has passed. Likewise, the imitation game does not provide us with a definition as to what thinking and intelligence is, but still provides us with a practical way of determining whether something is intelligent or not. So, is that what Turing proposed?
One immediate thing to note about this interpretation of the Turing Test is that it cannot be an operational definition of 'intelligence' in general, since any entity (machine of otherwise) whose intelligence is quite different from a human would never be able to pass the Turing test. In fact, the Turing Test has sometimes been criticized for exactly this fact: the test is seen as too 'anthropocentric', since through its set-up, it takes human intelligence as the 'standard' of intelligence. On the other hand, it is of course human-level intelligence that we, as humans, are most interested in. Indeed, note how we have spelled out the First War in these terms as well: Is it possible for a machine to have human-level intelligence? So, maybe the Turing-Test is an operational definition of human-level intelligence?
Here is what Turing writes:

May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if, nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by this objection.

This short passage shows us two things:
To Turing, the interesting case is where the machine does pass the test. If it doesn't, Turing doesn't want to draw any conclusion at all. Indeed, couldn't a machine have a means of communication that is completely different from us? Such a machine would still not pass the Turing Test, even if it would have 'human-level' intelligence. In other words, we cannot say that a machine has (human-level) intelligence if and only if it passes the Turing Test. Therefore, Turing is clearly not proposing any operational definition of intelligence, human-level or otherwise. We can at best say that a machine has (human-level) intelligence if it passes the Turing Test.
At the same time, Turing considers machines having some form of intelligence other than humans to be a 'strong objection'. We interpret this as Turing indeed being troubled that this test can only focus on human intelligence. That is, Turing certainly would like to be able to have a discussion on the nature of intelligence that is not exclusively focused on human-level intelligence. However, for now, Turing simply asks to consider what it would mean if a machine does pass the Turing Test.

OK, so suppose we have a machine that passes the test. Does Turing want to say that it is then intelligent? There is some logic to this. Rocks and salad shooters will clearly not be able to pass the test. Why not? It's because they cannot think and are not intelligent. That is, it would seem that intelligence is required in order to pass the test. But that means that if one does pass the test, one is intelligent. Or, to be more precise: To pass the test requires 'human-level' intelligence, so anything that passes the test has 'human-level' intelligence, and is therefore intelligent.
This is how many commentators interpret Turing's Test. And, many commentators will subsequently criticize the claim that anything that pass the Test is intelligent. 
[Bram: Normally one would have a lengthy discussion on criticisms and counter-criticisms at this point. But I really don’t want to … because that already would seem to be seeing the Turing Test into something that I will argue it is not!]. 
Some of the criticisms highlight the fact that Turing did not lay out many details about his Turing Test. For example: Who is or who are the interrogators? Who is or are the humans in the Game? How long of a conversation will the interrogator have with the human(s) and the machine(s)? What is the conversation about? What should be the rate of correctly identifying the machine from the human, below which we would declare the machine to have passed the test? With so many of these details not spelled out, it does indeed seem quite possible that something could 'slip by' and 'fool' enough of the interrogators to pass whatever threshold one happened to have set for passing the Test.
One could, of course, acknowledge these criticisms, but say that if an entity passes the test, there is nevertheless a considerable likelihood that it is intelligent. As such, we can compare the Turing Test with a kind of 'duck test': If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck. Likewise, Turing's point is that if something behaves in a way intelligent humans behave, then it probably is intelligent itself?
This, however, would seriously water down the scientific usefulness of the test. Moreover, it is no excuse for being sloppy with the criteria: the more specific the criteria, the more scientifically legitimate and thus more scientifically useful the test will be. And, the more stringent the criteria, the more probable it is that we are dealing with an intelligent entity: an entity that is able to convince more judges that it is a human, and after longer conversations with each, is more likely to be intelligent. 
However, scientifically useful or strict criteria seemed not to be on Turing's mind. In fact, unlike his highly analytic and scientific work on Turing-machines, Turing does not seem to be very precise about his Turing Test at all. Without specifying these kinds of details, Turing left any attributions of intelligence to a machine on the basis of his Turing test to be quite subjective.
Indeed, the lack of details and stringency is probably just as well. 'Intelligence', even if limited to 'human-level' intelligence, is a vague notion. Turing already argued that trying to define it would not be a very fruitful way to think about it. But for that very same reason, trying to capture it by specifying strict details about the Turing Test is equally pointless. Is there any scientific significance to having a 20-minute conversation rather than a 10-minute conversation as far as attributing intelligence goes? Would there be any special significance to the claim that at least 30 percent of the jury needs to be convinced as opposed to 35 percent? Of course not. Any attempt to specify precise conditions for 'passing' in the Turing Test will seem like an arbitrary precision: the fuzzy nature of 'human-level intelligence' simply defies any such hard delineation. After all is said and done, one can still debate and argue whether it is intelligent or not, or how probable it is. It is therefore quite appropriate, and was most likely quite intentional, for Turing to not specify too many details of the test. 
This, however, leaves us with a serious question: What good then is the Turing Test? What, indeed, was its point?
One interpretation of the Turing Test is not so much as a practical test, but rather as a way of expressing how Turing things we should go, and not go, about trying to deal with the question Can machines Think?
In a 1952 discussion with Max Newman, Richard Braithwaite, and Geoffrey Jefferson, broadcast on BBC Radio, Turing said:

I don't want to give a definition of thinking ... ... I don't really see that we need to agree on a definition at all. The important thing is to draw a line between the properties of a brain, or of a man, that we want to discuss, and those we don't. To take an extreme case, we are not interested in the fact that the brain has the consistency of cold porridge. We don't want to say 'This machine's quite hard, so it isn't a brain, and so it can't think.' I would like to suggest a particular kind of that one might apply to a machine. You might call it a test to see whether the machine thinks, but it would be better to avoid begging the question, and say that the machines that pass are (let's say) 'Grade A' machines. The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonable convincing. A considerable proportion of a jury, who should not be expert about machines, must be taken in by the pretence. They aren't allowed to see the machine itself - that would make it too easy. So the machine is kept in a far away room and they jury are allowed to ask it questions, which are transmitted through to it: it sends back a typewritten answer.
... Well, that's my test. Of course I am not saying at present either that machines really could pass the test, or that they couldn't. My suggestion is just that this is the question we should discuss. It's not the same as 'Do machines think', but it seems near enough for our present purposes, and raises much the same difficulties.

We see how Turing repeats the suggestion that we should not address the question of machine intelligence through a definitional approach. Indeed, Turing is hesitant to call the machines 'thinking machines', as he fears that this will go back to endless discussions about what intelligence is, and whether if something that passes really is intelligent or not. We also see, however, one important point that Turing is trying to make with his Imitation Game: that we should also not be distracted by irrelevant properties such as what a machine is physically made of, but that instead we should be looking at the machine's overall behavior. 
There is an interesting analogy here with 'life'. Some will insist that 'life' has to involve 'organic', i.e. carbon-based chemistry. As such, 'Artificial Life', which is not based on organic chemistry, would immediately be ruled out as 'genuine' life. But why couldn't there be non-carbon based life? The insistence that life be carbon-based seems to be a case of carbon-chauvinism? Likewise, one can interpret Turing's thinking here as a call to be not be carbon-chauvinists about intelligence: the fact that a machine is made of metal or plastic should not be used to declare that it cannot be intelligent.
This still leaves a bit of a question though: if Turing's point was merely that we should not be distracted by the fact that a machine is made of metal, then why bring up the Turing Test? Couldn't he have said that more directly? Moreover, all the points here seem quite negative: this is what we should not be doing, or here is something we should avoid doing in declaring a machine is not intelligent? Isn't there a more positive point to the test?
Turing himself was well aware of the limitations of the test if one were to actually run one. At some point during the 1952 BBC interview, Richard Braithwaite suggests that humans seem to have emotional responses to fruitfully deal with situations. For example, fear might signal a danger that requires immediate conscious attention, and a tantrum might fulfill the function of escaping responsibility. So, might not a machine need to have such responses as well in order to have a similar level of intelligence as humans? Turing responds:

Well, I don't envisage teaching the machine to throw temperamental scenes. I think some such effects are likely to occur as a sort of by-product of genuine teaching, and that one will be more interested in curbing such displays than in encouraging them. Such effects would probably be distinctly different from the corresponding human ones, but recognisable as variations on them. This means that if the machines was being put through one of my imitation tests, it would have to do quite a bit of acting, but if one was comparing it with a man in a less strict sort of way the resemblance might be quite impressive.[emphasis added]

In other words, if we run a strict Turing Test with the explicit task of differentiating between human and machine, then the interrogator might well be able to pick up on all kinds of behavioral differences between man and machine that have nothing to do with differences in intelligence between the two. But making such fine-grained distinctions in the behavior between human and machine is not what the goal should be in the Turing test: what matters to Turing, is that a machine is like a human in the kinds of abstract ways that are relevant to intelligence. That is, if machines can be like humans in all the ways that matter for intelligence, then if we declare the human to be intelligent, then we should likewise declare the machines to be intelligent. That is, 'intelligence' is vague, but it is nevertheless something we attribute.
There is a lot to be said for this interpretation. As humans, for example, we attribute intelligence to each other in much the same way as the Turing Test would do: instead of putting probes into each other's brains, we base our judgments and attributions of intelligence on the interactions we have with each other. And of those, it is largely our verbal interactions we find the most 'telling'. In real life, we feel we can make a fairly good determination of one's 'overall intelligence' after just a few minutes of conversation, even as we are hard-pressed to say what intelligence really is. Again, 'intelligence' is a vague notion, and our standards are accordingly not very scientific. However, we should at the very least be fair in the attribution of 'intelligence' to machines: whatever standards we use to attribute 'intelligence' to our fellow humans, we should hold the machine to the same standards. 
The set-up of the Turing-Test is often explained along these lines as well: making the machine invisible to the interrogator, and having the interrogator compare and contrast the answers from the machine with those of a human would indeed seem to ensure a common standard. To follow one of Turing's examples: if we were to interact with a machine, and asked it to write a sonnet, and the machine responded by not knowing how to write a sonnet, we should not hold this against the machine's intelligence, as many humans would not be able to do this either. 
However, rather than seeing this specific set-up as a practical consideration for running an actual test of intelligence, the point is just that we should not be biased and prejudiced against the machines. Likewise, Turing stated that any jury 'should not be expert about machines', as experts might be able to pick up on these differences more easily. But, rather than seeing this stipulation of 'not having experts' about the test as an actual practical consideration for when one actually runs the test, it is the idea behind it that is what is important: the emphasis should be on how much alike machines and humans are in terms of the kinds of things that matter for intelligence, not on how they are different. 
No actual Turing Test need to be run to make those very points. And, as we saw, and as Turing probably realized, running an actual Turing Test to try and declare a machine to be intelligent still has many problems that aren’t solved by having a human and a machine shielded from 'non-expert' interrogators. Again, then, it is unlikely that Turing seriously considered his test as an actual practical test. Rather, his Imitation Game was much more of a thought experiment, with the point of demonstrating to people that when it comes to attributing intelligence, we should be fair and consistent in doing so.
1.Despite the inherent lack of details of the test, and despite Turing likely not even being serious about developing any actual practical test, there have been a number of times where computers have been subjected to some kind of 'Turing Test' with the details of the test and what it means to pass it spelled out in detail. The most famous of these is the Loebner Competition, which has been held annually since 1990. In a typical competition, several judges (the 'jury') each interact with several humans and several computer programs, and the judges rank-order the entities they interact with from 'most likely to be human' to 'least likely to be human'. Hugh Loebner has offered a prize of 25,000 for the first program that judges cannot distinguish from a real human. The judges have typically been a mix of people working in academics, journalists (so they can write about it), and the occasional 'well-known personality'. Over the years the conversations have had different durations, from 5 minutes to 20. Here is a typical transcript (Do-Much-More was the best performing computer entry' in 2009):
Judge-1: Hi, how is it going?
Do-Much-More: Hello. How are you doing?
Judge-1: Good, who are you?
Do-Much-More: Well, I am studying engineering. That ought to give you an idea about who I am.
Judge-1: No, not really?
Do-Much-More: I have a terrific yearning to know something more about experiments?
Judge-1: Where are you from? 
Do-Much-More: I don't know, but don't forget I can't be in three places at once.

So, how have the machines fared in the Loebner test? Well, so far no machine has won the Loebner prize. Which is just as well, since few people would seriously consider the machine entrants to be 'intelligent'. 
Almost all entrants to the competition are a class of computer program called 'chatterbots' or 'chatbots': programs that are narrowly specified to uphold a conversation with a human as long as possible. As the example above demonstrates, 'chatter' is just about the right description: conversations feel unstructured, unfocused, and even individual responses feel like coming from someone who is just mouthing words to pass their turn in the 'conversation'. 
One of the first chatbots, and certainly one of the best known one, was Eliza, who pretended to be a psychotherapist. Eliza basically worked by looking for certain keywords, and giving one of several pre-programmed stock responses in return. Sometimes Eliza would throw in a simple "tell me more" or "I don't understand". Other times Eliza would simply repeat much of what the human on the other end of the line was saying, but turn it into a question. 
Indeed, the strategy of asking questions is one employed by many of these chatbots, as it kind of puts them in 'control' of the conversation, or at least avoids having them from actually answering questions or giving meaningful responses. Another common strategy is to intentionally make spelling mistakes in their responses, trying to make the interrogators think "Machines don't make those kinds of mistakes, so this is a human." Other chatbots try to take full advantage of a certain persona that they are supposed to have in order to 'excuse' any non-sensical answers. Of these, Parry, a program that pretended to be a human with paranoid schizophrenia, was the clearest example.
The fact that none of these bots has passed the Turing Test would seem to be suggestive that the Turing Test is a good test: maybe it does take some genuine intelligence to pass the test. However, some of the entrants in the Loebner competition have come pretty close. Indeed, there have been plenty of times where individual members on the jury did rank a computer above a human in terms of perceived 'humanness'.
In fact, in 2014, in a competition held at the Royal Society in commemoration of the 60-year anniversary of Turing's death, Eugene Goostman, a chatterbot that pretends to be a 13-year old Ukrainian boy, fooled 33% of the judges into thinking it was human. One of the organizers, Kevin Warwick, a noted researcher in Artificial Intelligence, robotics, and cybernetics, considered Eugene Goostman to have passed the Turing Test on the basis of Turing's prediction that “in 50 years’ time, judges will have no more than 70% chance of correctly identifying who is human and who is machine”. However, rather than the AI community celebrating the 'birth of human-level AI', much of the community was quite dismissive of Eugene Goostman's supposed 'intelligence'. And rightly so. Consider, for example computer scientist Scott Aronson's little chat with Eugene:

Scott: Which is bigger, a shoebox or Mount Everest?
Eugene: I can’t make a choice right now. I should think it out later. And I forgot to ask you where you are from…
Scott: How many legs does a camel have?
Eugene: Something between 2 and 4. Maybe, three? :-))) By the way, I still don’t know your specialty – or, possibly, I’ve missed it?
Scott: How many legs does a millipede have?
Eugene: Just two, but Chernobyl mutants may have them up to five. I know you are supposed to trick me.
Scott: No, I need to know that you’re not a chatbot. Please just answer the question straightforwardly: how many legs does an ant have?
Eugene: Something between 2 and 4. Maybe, three? :-))) Oh, what a fruitful conversation;-)
Scott: Do you understand why I’m asking such basic questions? Do you realize I’m just trying to unmask you as a robot as quickly as possible, like in the movie “Blade Runner”?
Eugene: … wait
Scott: Do you think your ability to fool unsophisticated judges indicates a flaw with the Turing Test itself, or merely with the way people have interpreted the test?
Eugene: The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later.

Whatever 'human-level intelligence' is, it is abundantly clear that Eugene Goostman does not have it. So, why did many of the interrogators of Eugene think it was a human? We can only guess here, but in addition to Eugene's conversational tricks, there may also have been a good bit of anthropomorphizing going on. That is, in order to explain the behavior of other things, we are quick to attribute human qualities to non-human objects, as doing so would allow us to hopefully correctly (though often wrongly!) predict other behaviors and properties. Thus, with a little luck, and with some of the interrogators probably not questioning as hard as Scott Aronson, but instead trying to actually converse with Eugene as one would with an actual human being, the responses of Eugene Goostman were plausible enough that the interrogators extrapolated and induced those human qualities.
The case of Eugene Goostman and other modern-day implementations of the Turing test confirm why it is indeed a bad idea to try and have any actual, practical, Turing tests. Given how far away these chatterbots are from what anyone could seriously consider human-level intelligence, but how relative easy it apparently is to convince a human interrogator otherwise (at least in the span of 10 minutes or so) is a serious strike against trying to actually implement the test as an actual text of intelligence. Seeing the criteria of what it means to pass a Test are seen as arbitrary, and when a machine does satisfy those criteria, the debate as to whether it is intelligent does not go away.
OK, but if not a test, could the Turing test maybe still form some kind of interesting Grand Challenge in AI, the way that beating the best human chess player, or engineering a self-driving car were interesting (and fruitful) Grade challenges? Here are some reasons it is not. 
First of all, the Turing test is really quite different from something like beating the best human player in chess, Go, or Jeopardy. In the latter, there is a fairly well-defined criterion for success or failure. But displaying 'general human-level intelligence' is so vague that one cannot expect for there to ever be some specific and momentous occasion to which we can point and declare that some machine has done it. Thus, we'd be hard-pressed to say when the Challenge has been achieved, and that takes a lot of the excitement out of it, which seems like a necessary ingredient of a Grand Challenge. 
Second, a good Grand Challenge is one that is within reach: one that pushes the frontiers of science, technology, and engineering, but without breaking them. Going to the moon was a great Grand Challenge; going to Alpha Centauri is, as of yet, not. Similarly, trying to obtain a machine with human-level intelligence is not. We are still too far away from it: it is still too 'grand' of a grand challenge. And indeed, the attempts to 'pass' the Turing test have resulted in no interesting breakthroughs in the field of AI at all. It is still too hard, and hence we are resorting to the kind of cheap and superficial 'trickery' that we see in the modern day Turing Tests. 
Indeed, the problem with modern-day Turing Tests is that they encourage the goal of 'passing the test', i.e. trying to make the interrogator think they are dealing with a human, rather than trying to create a genuinely intelligent machine: they focus on exactly what Turing wanted us not to focus on. And, as Turing feared, even if a machine passes the test, then people will continue to debate whether it is really intelligent or not.

Indeed, it seems like Turing was much more concerned with trying to make sure that we are not too quick to deny intelligence to a machine, than that we might be too quick to grant intelligence. This actually makes sense, given the historical context of his work. Turing was one of the first people to seriously consider the idea that machines could be intelligent, and had to fight the general associations that people have with machines that told them otherwise. Long-time friend and fellow mathematician Robin Gandy describes the circumstances under which Turing wrote his 1950 paper as follows:

The 1950 paper was intended not so much as a penetrating contribution to philosophy but as propaganda. Turing thought the time had come for philosophers and mathematicians and scientists to take seriously the fact that computers were not merely calculating engines but were capable of behavior which must be accounted as intelligent; he sought to persuade people that this was so. He wrote this paper - unlike his mathematical papers - quickly and with enjoyment. I can remember him reading aloud to me some of the passages - always with a smile, sometimes with a giggle. Some of the discussions of the paper I have read load it with more significance than it was intended to bear

Our interpretation of this passage is that Gandy is reacting to the many, many papers that consider the Turing Test as a practical test, discuss its perceived values and shortcomings, and suggest possible improvements, replacements, or adaptations of the test. Gandy believes they all take the Turing Test too seriously as an actual practical test, and so do we. Instead of being a practical test, the Turing Test is better seen as a kind of cautionary tale against initial prejudices one may have regarding what can, and what cannot, be intelligent. As such, attempts to 'fix' the Turing Test for its perceived shortcomings as far as determining whether something is intelligent are therefore completely beside the point. As we saw before, 'intelligence' just doesn't lend itself to any practical test. And if Turing had wanted to have a test with any kind of scientific validity, Turing could have chosen from a number of tests already designed by psychologists.
Instead, we want the reader to consider the story of Mulan, where a woman dresses as a male in order to fight in the army. Had Mulan not hidden the fact that she was a woman, she probably would not have been accepted in the army, the reason being given that 'women can't fight as well as men'. Mulan, however, turns out to be a skilled warrior, so when she is later revealed to be a woman, everyone who initially thought that women can't fight have to eat humble pie! It is possible that Turing had something similar in mind: that through his Turing-test, he could serve the nay-sayers a slice of humble-pie. 
In 1948, Turing wrote a report for the National Physical Laboratory, called 'Intelligent Machinery'. In a section called 'Intelligence as an emotional concept', Turing writes:

The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behavior.”

Several years later, in the 1952 BBC Radio discussion with Richard Braithwaite, Geoffrey Jefferson, and Max Newman, Turing stated this point as follows:

Braithwaite: But could a machine really do this? How would you do it? 
Turing: I've certainly left a great deal to the imagination. If I had given a longer explanation I might have made it more certain that what I was describing was feasible, but you would probably feel rather uneasy about it all, and you'd probably exclaim impatiently, 'Well, yes, I see that a machine could do all that, but I wouldn't call it thinking'. As soon as one can see the cause and effect working themselves out in the brain, one regards it as not being thinking, but a sort of unimaginative donkey-work.

Turing thus points his finger at a particular kinds of preconceived notion that many people to have, which is that 'intelligence', whatever it is, must be something rather 'special' and 'complicated'. That is, some people that intelligence is some 'thing' that some entities have, and others not, and that this 'thing' has to be inherently complex and complicated. Indeed, one can make an analogy with 'life' here, where for a while people thought that 'life' was some 'special ingredient' that some things have, and others not. Of course, science has shown that all aspects of life can be explained in terms of underlying biochemical mechanisms or, as Turing would say, 'unimaginative donkey-work'. The same, Turing predicts, will be found for 'intelligence': it is not some 'mysterious' and 'magical' ingredient that some have and others don't, but rather the result of more 'unimaginative donkey-work' or, as Turing would have it, computations. However, as long as some people perceive 'intelligence' as pretty much the opposite of 'unimaginative donkey-work', Turing warns that people will be unwilling to attribute intelligence to machines, unless we somehow hide the fact that the person is dealing with a machine in the first place. Indeed, Turing continues the 1948 passage as follows:

It is possible to do a little experiment on these lines, even at the present stage of knowledge. It is not difficult to devise a paper machine which will play a not very bad game of chess. Now get three men as subjects for the experiment A, B, and C. A and C are rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.”

This, of course, is just like the story of Mulan: If C would know the opponent is a machine, C may well have been quite unwilling to say that the machine is playing a good game of chess. But not knowing the opponent is a machine, C is probably happy to say that B is a decent chess player.
Moreover, it is clear that the 'little experiment' here was a forerunner to the more general Turing Test. However, it is equally clear that Turing does not regard this 'little experiment' as any kind of test at all, but rather as something to illustrate the prejudice against the possibility of machine intelligence. If Turing's account of the Imitation Game two years after his 'little experiment' is still making the same point, then it is clear that Turing did not mean the Imitation Game to be a practical test, but rather as an illustration that the attribution of 'intelligence' is to a large extent subject to our preconceived notions regarding 'intelligence', and what we feel can and cannot be intelligent. As such, the Turing Test is again better seen as a thought experiment: a way to reveal and, like the story of Mulan, to correct our prejudices regarding intelligence. An experiment that, indeed, says more about us, and how we treat the idea of machine intelligence, than about machine intelligence in and of itself. Indeed, if anything is being tested by the Turing Test, it is our preconceptions regarding the notion of machine intelligence.
1.Of course, in the end, whether Turing meant his test to be an actual practical test or not, is a question best left to historians and biographers. If Turing did propose his test as an actual practical test, then he may well have underestimated the power of anthropomorphizing. Indeed, a current concern with Artificial Intelligence is that humans are attributing more human-level cognitive powers to them than is warranted, as that could lead to some dangerous issues of trust: we might trust the AI systems to be able to do more than it really can. This debate, of course, will be taken up in more detail in part IV of the book.
However, while anthropomorphizing is a concern as far as attributing intelligence goes, it is also true that there comes a point where the attribution of intelligence is completely unproblematic. Suppose, for example, that we ever run into or are able to create something like Commander Data from Star Trek: The Next Generation. It is clear that after interacting for a couple of days, if not just a few hours, or even minutes, we will not, and should not, have any reservations about calling it 'intelligent'. We may not know what 'human-level intelligence' is, but it is clear that Commander Data has it, just as it is clear that Eugene Goostman does not. Now, did we just run a test on Commander Data? Well, yes and no. No, we certainly did not run a test with the whole setup of trying to figure out whether we are talking to Commander Data or a human. And, in fact, were we to do so, we most likely would reveal Commander Data not to be a human being. But yes, in the less strict sense of the test, we have been putting Data to the test: we interacted with Data, and on the basis of his behavior, attributed intelligence to him, just as we would for any human being. With his Imitation Game, Turing really wasn't saying anything significantly more than this.
In fact, Turing believed that as far as the day-today usage of the word 'intelligence' is concerned, people would certainly come to adopt to apply the term to machines. In his famous 1950 paper, he writes:

The original question, 'Can machines think?' I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.

We believe that history has largely vindicated Turing on this score. When our computer is taking some time before it comes back with an answer, we say it is 'thinking'. When computers adjust their behavior on the basis of past interactions with the user, we say it is 'learning'. We have 'smart phones' and other forms of 'smart technology'. And even when we say that our computers did not understand something, or did something 'stupid', note that we don't really say this about salad shooters: salad shooters are not even a candidate for possibly understanding or doing 'smart' things ... but we see computers as those candidates. Indeed, we regard Turing's claim above as remarkably prophetic.

A historically important section of Turing's 1950 paper is called 'Contrary Views on the Main Question', in which Turing considers objections to his view that eventually we 'will be able to speak of machines thinking without expecting to be contradicted'. Roughly put, Turing is considering a list of objections against the very idea of machine intelligence, and provides counterarguments to each. 
In doing so, Turing occasionally makes reference to the imitation game in order to illustrate the point that intelligence is something to be attributed, and that we should not be guided by our preconceptions as to what intelligence is, how it comes about, and what we believe machines can or cannot do or be. As such, the imitation game is used as a thought experiment to discuss such preconceptions and biases. Indeed, had Turing meant for his Turing Test to be an actual test of intelligence, one might have expected Turing to defend his test from some of the obvious criticisms when used as such, but Turing does no such thing. Again, the point of his paper was not to introduce an actual test for intelligence, but to defend the very thought that machine intelligence is possible, and to ready the world for this to happen.
In the next few sections, we will discuss some of the main objections that Turing anticipated. It should be noted that many of these very same objections that are still popular nowadays. 

The objection here is that machines can never become conscious. They may be able to do all kinds of things, but they cannot feel or experience anything and, as such, should not be declared 'intelligent' or, maybe more to the point, have a 'mind'.
Turing's response to this objection is multi-pronged. First of all, Turing points out that it is not a given as to whether machines can be conscious or not. Indeed, he regards this sentiment as one of the preconceptions we have about machines: that we see them as 'just' bits of metal and plastic. Second, Turing points out that it is difficult to determine whether something is conscious or not. Indeed, we may not be able to determine whether a machine is conscious, but the same is true for humans: we cannot tell of each other whether we are conscious or not. As such, Turing questions the need for figuring out what consciousness is, and whether it can be replicated in a machine, in order to give an answer to the question of whether a machine is intelligent:

I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localize it. But I do not think these mysteries need to be solved before we can answer the question with which we are concerned in this paper.

Here, Turing makes good use of his imitation game: if a machine's responses are essentially the same as ours, does it really matter whether it is conscious or not? We can ask whether Commander Data from Star Trek is really conscious, but in the end, Commander Data is clearly intelligent.
In an earlier discussions regarding the use of the word 'intelligence', we decided a good use of the word 'intelligence' (and much else of the other mental and cognitive vocabulary) is to ascribe it to entities in order to explain and predict their behavior. For that, we do indeed not need to solve the question of consciousness, or even know whether something is conscious or not.
Turing writes:

These arguments take the form, 'I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X'. Numerous features X are suggested in this connexion. I offer a selection:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humour, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make some one fall in love with, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.

Once again, Turing has a number of responses. Turing points out that some of these 'disabilities' seem to be rather irrelevant as far as 'intelligence would go, such as 'enjoying strawberries and cream', while others, such as learning from experience, are just plain false, as there are learning machines. Turing also points out that many of these objections may well be generalizations from past and existing machines, but that does not mean that there is no possible machine that can do any of these things:

No support is usually offered for these statements. I believe they are mostly founded on the principle of scientific induction. A man has seen thousands of machines in his lifetime. From what he sees of them he draws a number of general conclusions. They are ugly, each is designed for a very limited purpose, when required for a minutely different purpose they are useless, the variety of behaviour of any one of them is very small, etc., etc. Naturally he concludes that these are necessary properties of machines in general.
What Turing called the 'Lady Lovelace Objection' objection is the objection that machines can only do what they are told to do. Lady Lovelace effectively made this objection when she wrote that Babbage's Analytical Engine could not originate anything, and can only do 'whatever we know how to order it to do'.
This objection is still a very common objection nowadays, and there are a number of variants:

Machines cannot make mistakes
Machines cannot do anything new or novel
Machines cannot be creative

In response to the 'machines do not make mistakes' objection, Turing writes:

It seems to me that this criticism depends on a confusion between two kinds of mistake. We may call them 'errors of functioning' and 'errors of conclusion'. Errors of functioning are due to some mechanical or electrical fault which causes the machine to behave otherwise than it was designed to do. In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. The machine might, for instance, type out mathematical equations, or sentences in English. When a false proposition is typed we say that the machine has committed an error of conclusion. There is clearly no reason at all for saying that a machine cannot make this kind of mistake.
Turing thus points out that we can take two different perspectives on the 'behavior of machines'. There is the 'low' level of underlying mechanics, and the following of individual instructions ('functioning'), and then there is the 'high' level of overt behavior, as when a machine answers a question or makes a claim ('conclusion'). These two levels, or perspectives on what it is the machines does, are crucially different. For example, a calculator could be programmed so as to consistently give us the wrong answer to the question as to what the sum of two numbers is, even as it is flawlessly following the instructions of its program. 
Assuming our brains are what underlies our mental capacities, the same may well be true for humans: we make plenty of thinking mistakes, even as our neurons fire in perfect congruence with the laws of biochemistry. In addition, for the purposes of explaining and predicting mental capacities, each perspective has their own advantages and disadvantages, depending on exactly what it is one is trying to explain or predict. Usually, when trying to explain a person's behavior, the explanation will be pitched at a fairly 'high' level, and only when something goes wrong (e.g. the person suffers from some mental condition), do we opt to go down to the level of neurochemistry. Similarly, in explaining the behavior of a machine, we rarely go down to the level of individual instructions. But when we say that 'machines do not make mistakes', we probably have that low level in mind, which is not the level we use to evaluate typical human behavior.
Turing also points out that:

The criticisms that we are considering here are often disguised forms of the argument from consciousness. Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that the method (whatever it may be, for it must be mechanical) is really rather base.
So here, we see that Turing is making a similar point as the one of his 'little experiment' of 1948: that once one describes the method by which something is accomplished, people will often cease to consider it 'intelligent'. People believe that there needs to be something more than just 'donkey-work'; people feel there needs to be something 'special' and more 'mysterious'. Something that is, in and of itself, reflective of 'intelligence'. This, however, follows a common pattern of fallacious thinking, sometimes called the fallacy of composition or division: There is a tendency for people believe that some high-level property can only be explained by low-level properties that 'reflect' that high-level property. Water, for example, was seen to be composed of little wet and transparent water molecules, and life requires some kind of special 'vital force' that living things have inside of them, that has left dead things, and that will never inhabit inanimate objects at all. Modern science, however, has shown that macro-level properties can 'emerge' out of low-level properties. Indeed, if the micro-properties that are used to explain the macro-properties are of the same kind as those macro-properties, then we are still left to explain those very kinds of properties.
The same distinction between these different levels or perspectives can be used to address the objections that machines cannot learn, be creative, or do anything new or novel. Again, it is true that at the level of functioning, the machines cannot deviate from their internal mechanics, but judgments as whether something learns, is creative, or does something novel is typically made at the macro-level of overtly observable behavior. In fact, we have machines that learn, and machines that compose new music, novels, or works of art.
Finally, we can bring in Turing's earlier point about us making generalizations from the machines we have encountered in our lifetimes. Most machines we see around us have been specifically designed to do a very specific task, and to do that task without mistakes, especially at the behavioral level. Indeed, we have many reasons to not want a machine to produce unpredictable behavior. As such, it is indeed natural to conclude that all machines are like that. But, as Turing points out, they do not have to be like that.

The Turing Test paper was an important landmark in the philosophy of AI, for several reasons:
 Turing makes the important point that 'intelligence' does not have a precise definition, but is something we nevertheless attribute to things in order to make sense of the world. 
Turing states that in such attributions, we should not treat machines any differently from humans:

We should not generalize from existing machines to make claims about what machines possibly can be like.
We should not focus on the internal level of 'functioning' of computers, which most people will find to be mere 'donkey-work', and thus declare 'unintelligent'. Instead, when it comes to the attribution of 'intelligence', we should focus on the level of overall behavior, just as we do for our fellow humans.
We should not have long discussions as to whether machines can be conscious or not. We don’t know whether our fellow humans are conscious either, and for the attribution of intelligence, it doesn't matter.

Turing's presents his Imitation Game ('Turing Test') as a thought experiment to emphasize the previous points.
Turing makes one of the first bold assertions that computers can be intelligent.
Turing anticipates and counters many of the objections to the claim that machines can be intelligent.
Turing makes a remarkably prophetic prediction that around the year 2000, we would indeed start applying the cognitive vocabulary to describe and predict the behavior of machines. 
2. A (very!) Short History of the Philosophy of Mind

It is impossible to have a philosophical discussion on the possibility of machine intelligence without a discussion of the longstanding philosophy of mind. Unfortunately, it is also impossible to cover the whole of the philosophy of mind given the limited space of this book. We shall therefore present what is a necessarily brief overview (pretty much a 'cartoon' overview) of the main positions, especially as it relates to 'intelligence'.
One of the oldest positions on the nature of mind is that of Classical Dualism. Classical Dualism states that our mind is a non-physical entity that controls our body. Indeed, if it wasn't for our mind, our body wouldn't do a whole lot of interest, and maybe just nothing at all. In fact, historically, Classical Dualism was pretty much part of the view of animism, which states that if anything physical moves, it is because a non-physical mind is making it move. 
One can understand the intuition behind this view. A pile of rocks just sits there, doing nothing. We humans, however, do things. And, we often find ourselves doing things because we want to do those things: I have the thought or desire to go somewhere, and so I stand up and do exactly that. And even if I don't, it may still be my mind that tells me that it is better to stay put. Sometimes I have no conscious thoughts, such as when I sleep. But notice that when I sleep, I don't *do* anything either. So, everything from our experience tells us that we do things because of our minds. And if this is true for us, the reasoning goes, then that must be true for other things as well. A pile of rocks, never doing anything, has no mind. Animals move, so they do have a mind. Also, small things and simple events require only simple minds. But big things, and big events, obviously require big minds: Gods. Indeed, in classical times, we believed the world was suffused with spirits, ghosts, souls, and Gods. Our own mind was just one of those.
However, what is the nature of this mind that makes the body move? We never seem to see those ghosts, spirits, and ghosts. Even as 'plain' human beings, we don't see each other's minds? And when the early scientists opened up humans and animals, they found nothing that looks anything like a conscious mind. So again, what is the nature of these 'minds' and 'spirits'? Well, the fact that we can't observe it like the physical world around us, must therefore mean it is something non-physical. Indeed, it is something akin to a 'ghost'-like soul or spirit. Something that can inhabit physical objects and, as such, control the movement of those physical bodies. But the time at which the spirit leaves the body, is the time where the body ceases to function: this is when we die. But this death of the physical body does not mean the end of the spirit. As many religions and cultures believe, we can survive bodily death and can go to a different place or inhabit a different body.
On this classical view then, all these notions are basically one and the same then: mind, spirit, soul, ghost, consciousness, life, and even myself. I am the captain of the ship, with my body as my vessel.
As science progressed, we started to believe that physical things can cause pother physical things: that the physical world can uphold its own chain of events, movements, and dynamics, without the help of any non-physical minds. More and more complicated gadgets and pieces of machinery were created, whose workings could all be explained by reference to their internal mechanisms. Indeed, our world view became a lot more mechanical. By the time of the 17th century, many minds, spirits, and even many Gods, were being squeezed out of existence: they were no longer seen as necessary to explain behavior. In fact, by this time we had learned enough about bodily anatomy, and in particular our nervous system and our brain, that Rene Descartes believed that even the behavior of all animals could be explained by pure mechanics and biochemistry alone.
Still, the existence of conscious thought remained a riddle. In fact, as Descartes pointed out, the existence of conscious thought cannot be doubted, as the very act of doubting is a conscious act. Indeed, any time I have a conscious thought or experience, I can be certain of the existence of myself, as the mind who has those thoughts and experience: I think, therefore, I am. On the other hand, the existence of a physical reality, as we imagine it to exist independently of our thought, cannot be proven with such certainty: for all we know, an evil Genius is putting experiences of seeing elephants and salad shooters into my conscious mind, but there are no actual elephants or salad shooters. Even the existence of my own body, including my own brain, can be doubted. The only reason we believe these things exists, Descartes argued, is because we have experience of them. But if the existence of my own mind cannot be doubted, but the existence of anything physical is, then my mind has to be something non-physical.
Descartes' argument led to a view called Cartesian Dualism: like any dualist position, there is still a fundamental metaphysical distinction between mind and bodies: bodies are physical, whereas minds are not. However, at the same time, we see a move away from Classical Dualism, as with the advent of science it was recognized that some behavior could have purely physical explanations. In fact, even a 'simple' cognitive process like perception and action could be explained in purely physical, mechanical, neurological terms. However, 'higher-order' cognitive acts, like reasoning, decision-making, and planning, was still seen to require a deliberate and conscious mind. Thus, in contrast to the kind of 'one-way-street' classical dualism, where the non-physical minds could cause physical behavior, but not vice versa, in Cartesian Dualism, there is a two-way interaction between the mind and the body (brain): at times, the brain will 'signal' or 'communicate' to the mind that something is going on that requires our conscious attention and intervention, and once we have figured out what to do, the mind will communicate its decisions back to the brain, from where the appropriate neurochemical signals get sent to the motor control parts of our body to do what needs to be done. In sum, I am the ghost in the machine.
As science progressed further, more and more phenomena came to have a purely physical explanation. Even life came to be seen as merely an abstract set of behaviors and functionalities with a purely physical underpinning. Indeed, all of biology was seen as reducible to chemical processes, which themselves reduce to purely physical events. So, why should psychology, cognition, and the human mind be any different? Indeed, since the 19th and 20th century, a clear scientific picture emerged where a purely physical universe only gradually gave rise to more and more complex processes, giving rise to chemical, then biological, and finally psychological phenomena. On this picture, then, everything has a physical basis.
This view, called materialism or physicalism, thus clearly rejects a dualist outlook. Rather than there being two separate kinds of 'things' (physical and non-physical), there are really only physical things. The human mind may seem to be something that is not physical, but on the view of materialism, it is merely an abstract or macro-level perspective on what are ultimately physical underlying processes.
As an analogy, take the U.S. economy. We certainly talk as if the U.S. economy is a 'thing', and yet it is quite unlike things like chairs or salad shooters. The latter we can see, touch, and pick up. They have mass, a certain location in space, and can be painted a certain color. The U.S. economy doesn't have any of those features. Indeed, no matter how high we hover over the Earth, there will never be a point where we say "Ooh, there it is! The U.S. Economy!" Thus, the U.S. economy does not seem to be your typical 'physical' object. 
However, we all realize that what gives rise to the U.S. economy is a set of transactions: the trading of goods and services. And, those transaction and trades clearly have a physical underpinning. Indeed, no one is a dualist about the U.S. economy, claiming that its existence requires one to postulate non-physical, 'economy-like', kinds of entities. That is, we believe that ultimately, the U.S. economy is merely an abstraction of physical processes. 
The same is true, materialism claims, for minds: minds are not 'things' or objects. There are no spirits, souls, or ghosts. There are merely physical, chemical, and biological processes that are very complex, but give rise to systematic patterns at a macro-level that we recognize, conceptualize, and verbalize with our vocabulary related to minds, cognition, and intelligence.
OK, but exactly which physical processes give rise to minds, cognition, and intelligence? In answering this question, materialist theories have typically gone back and forth between two clusters of physical processes: the physical processes that take place within our body (and in particular, in our brain!), and the physical processes that take place as we interact with our environment (i.e. our overt behaviors). Indeed, in what follows, we will roughly trace the history of psychology as a scientific study of mind, and how it has gone back and forth between these two kinds of views.
We will start our discussion with the beginnings of psychology, which happened in the 19th century. Early psychologists often used introspection to study mental abilities and what underlies them. Thus, in a typical experiment, a subject would be put in a situation, or asked to perform some task, and the subjects would report on what they introspectively perceived was 'going on in their minds'. In fact, psychologists would often use themselves as the subject. 
Introspectionism was criticized on the basis of its measurements not being 'publicly observable'. It was therefore maybe not all that surprising either that the pendulum swung the other way, leading to Behaviorism.
How can we study the mind, and yet be good scientists? Behaviorists answered this challenge by focusing on what clearly is objectively observable: our overt, observable, behavior. Thus, in a typical behaviorist experiment, a subject would once again be placed in some environment, but this time it was the subject's overt behavior that was being recorded. Here, we are talking about things like reaction time, ability to solve problems, or ability to change behavior over time. 
It should be noted that the behaviorists gave a methodological answer to the question of how to best study the mind, rather than an answer to 'what is the mind'? Indeed, few behaviorists would say that they directly measured or quantified the mind by observing behavior, but they nevertheless believed that through measuring behavior, they could infer meaningful claims about minds as well, as the assumption is that the mind is responsible for that behavior. For example, a rat figuring out that pressing the left lever results in a food reward could be said to learn. 
Notice that in real life we typically make attributions of mental abilities, including intelligence, also on the basis of one's behavior. That is, we interact with fellow human beings, or other animals or, for that matter, a machine, and we are typically fairly quick in judging the mental capacities of this entity merely on the basis of those interactions. Indeed, even if we could go inside someone's head and look at their brain activity, that really wouldn't help us in determining their mental capacities, at least in this early stage in the study of neuroscience. 
In fact, some behaviorists took this line of reasoning to its extreme, and claimed that the mind really is the whole of one's behavioral dispositions. So, for example, to learn something, really is the mere fact that one is changing one's behavior. Or, to desire something is to act in certain ways in certain situations. Thus, while the methodological choice to focus on behavior when studying the mind is called psychological behaviorism, the view that all that behavior is the mind became the view of philosophical behaviorism.
As an analogy for their view, a philosophical behaviorist might point to a fast car and ask: what is the fastness of that car? Sure, it has a powerful engine, but that is merely what causes the car to be fast. The very fastness of the car is nothing more or less than its ability to be able to go from A to B in a short period of time. Likewise, the philosophical behaviorist argues, my brain is most likely causally responsible for me to be able to solve problems or remember things, but if you ask what my problem-solving mental ability is, you shouldn't be pointing to my brain, but rather to my ability to do the 'right' thing when put in certain situations. Or, as applied to 'intelligence': to be 'intelligent' is to be able to solve problems, give correct answers, do things that are to my advantage, etc. My behavior isn't just indicative of my intelligence. Rather, it is my intelligence.

One obvious drawback with philosophical behaviorism is that we seem to have a rich mental life without necessarily having any behavior associated with it. For example, I can be sitting in a chair thinking about some problem, yet not acting on it whatsoever. Surely we wouldn't say that I wasn't having any thoughts because there was no observable behavior! 
Considerations like these made the pendulum swing back again, and the focus returned to what goes on 'between our ears'. Indeed, probably bolstered by a slowly but steadily maturing neuroscience, the focus became on our brains which, thanks to that very neuroscience, and especially the emerging brain scanning techniques, we could begin to study scientifically. Indeed, mind-brain identity theory became the view that mental states are identical to brain states, rather than behavior or behavioral dispositions. Thus, for example, to remember something is to have a certain part of my brain configured in some way or other.
Mind-brain identity theory was criticized on two different grounds:
First, by literally equating mental states with brain states, it would seem to imply that one cannot have the same mental states without those very brain states. This, some critics noted, implies some kind of 'carbon chauvinism'. That is, might it not be possible for there to be intelligent creatures on other planets whose physiology is quite different from ours, yet who can hold the same beliefs we do? But if their brains have a completely different architecture from pours, then they can't have the same brain states.
Second, it seems that it isn't just what the brain state is (e.g. what it is composed of, and what neurons are involved), but what that brain state does, that is important. That is, what counts as far as brain states go, is how it relates to other parts of the brain, and how all of that is hooked up to the rest of the body through the nervous system, our perceptual apparatus, and our motor control functions. To use an analogy: the same bit-string inside a computer can represent completely different things, depending on how the rest of the computer uses that bit-string, i.e. how that bit-strings functions, and what role it plays relative to the rest of the interconnected system, including its 'peripheral' devices such as keyboard, mouse, monitor, etc.
Both of these observations form the foundation of the next theory if mind: Functionalism.
Functionalism agrees with the mind-brain identity theorists that we should clearly take into account our brain when trying to account for one's mental states and capacities. However, in a move more reminiscent of behaviorism, the functionalist will argue that rather than pointing to what the brain is, and what it is composed of, the key lies in understanding how the brain functions. Or, to put it crudely: where a behaviorist might say: 'mind is what body does', and where a mind-brain identity theorist would say: 'mind is what brain is', a functionalist would say: 'mind is what brain does'. As such, Functionalism can be seen as a kind of compromise between behaviorism and mind-brain identity theory. 
A functionalist takes the two criticisms leveled at the mind-brain identity theorists to heart, and makes them a core part of their view on the mind:
First, to a functionalist, it does not matter what specific material my brain is made of, or where in the brain some kind of brain activity takes place, as far as my mental states or cognitive capacities goes. The relevant functionality as far as mind and cognition goes is thus said to be multiply realizable. 
Second, we can't just point to something like the brain in isolation, and try and make sense of it. What matters to the functionalist, is the role that brain activity plays in the interconnected system of the brain as a whole, how the brain interacts with my body and the world at large.
To illustrate both of these ideas, a functionalist might, for example, point to a chair, and say: what makes that chair a chair? Is it the fact that it is made of wood? No, because we can have metal chairs as well. Is it the fact that it has 4 legs and a back rest? No, because there are chairs that have more or less legs and that don't have a back rest. What makes it a chair then, is the fact that it functions as a chair. That is, we can sit on it! 
Or, let's go back to the car that the behaviorist pointed at. To a functionalist, to give an account of the car being fast (or at least to explain it) does require one to point to the engine. But again, rather than pointing to what physical materials were used for the motor, or whether the motor is in the front or the back, the functionalist will point out how the motor behaves and functions and how that behavior and functioning, coupled with how the motor is attached to the driveshaft and the driveshaft to the axles, and the axles to the wheels, allows the wheels to be spun very fast. And finally, when those wheels are put on a surface like a road, the car will be fast.
A functionalist will say that our mental vocabulary can, and should, be attached to a highly complicated functional system involving our brain, our sensory input, and our motor output systems, and how all of that interacts with the world at large.
To say that our mental vocabulary is to be attached to some abstract functional organization feels like a pretty vague claim. Can we be a bit more specific about this? Which features of some actual organization, for example the brain, are relevant? The computationalist says that when it comes to the mind, cognition, and 'intelligence', the relevant functional organization is one that we typically associate with computational systems. That is, we know that we can build computers from different materials, and we can freely move around any components, as long as the abstract interconnectedness between the components remains the same. The size of the components doesn't matter either: I can build computers as big as a house, or as small as a pinhead, and have their computational abilities be exactly the same.
Notice that this does not work for chairs: That is, while 'chairhood' may well be a functional notion, it is clearly not a computational one. No program I run on my laptop, for example, will suddenly turn my laptop into a chair. And even if my laptop happens to be shaped like a chair, it isn't a chair due to what particular computations are running on my laptop. Thus, we see that computationalism is a strict sub-variety of functionalism.
Why do computationalist believe that our mental vocabulary, cognitive abilities, and words like 'intelligence' can be linked to computational architectures? Computationalist have several reasons for this.
A conceptual (i.e. a priori) reason for believing this is that computational architectures are information-processing architectures, and much of our mental vocabulary can be rephrased in terms of information-processing as well. For example, reasoning can be seen as the inference of new information from old information, memory is the storage of information, and perception the extraction of information from the environment. Planning and decision-making would be the use of information in order to figure out what to do next, and learning could be the result of incorporating newly obtained information in all this.
A second, more evidential, reason is that our mental abilities are clearly associated with our brain: changes to the brain are clearly linked to changes in one's mental capacities, and brain imaging techniques have even implicated specific parts of the brain that are involved with specific mental processes. But again, it is not what the brain is made of, but rather the functionality of the brain that the functionalist says is important, i.e. what it is that that part of the brain is doing in relation to the other parts of the brain, the rest of the body, and the environment. Computationalism simply goes one step further, and states that the functional processes that the brain realizes, and that give rise to minds, intelligence, and cognition, are computational processes: the brain is a kind of computer. 
To argue for the claim that the brain is some kind of computer, the computationalist can argue that the brain is an organ unlike any other organ. It doesn't filter, pump, or support. In fact, it doesn't seem to do anything 'physical' or 'mechanical'. But, we do see signals being sent to and from the brain through our nervous system. These signals, the computationalist argues, are just like the kinds of signals we find inside the electronic computer, in that they relay information. Indeed, where in the past the brain has been likened to a heat regulator, a control machine, telegraph system, and telephone system, we now have reached a metaphor that really seems to be quite apt: the brain is an information-processor, i.e. a computer of sorts.
A computationalist believes that having a certain computational architecture, whether implemented by the brain, a computer, or some other physical system, is sufficient for mental properties and cognitive abilities to be present. In fact, some computationalists will argue that it is necessary, i.e. that there is no way for cognitive capacities to come through anything other than a computational system. If we take the latter, stronger view, the claim of computationalism becomes testable: anytime we are faced with an entity that has cognitive capacities, computationalism predicts that there is some kind of computational architecture behind the scene. This could be an on-board computer, like some kind of brain, or maybe some external device that the entity is in communication with, but there has to be something computational that explains the entity's cognitive abilities. Indeed, if we ever were to encounter an intelligent race of aliens, then computationalism would admit defeat when we don't find anything like this when analyzing those aliens (if they let us!). 
In the absence of alien intelligent life, however, we can also just point to ourselves, and other animals, as evidence for the claim of computationalism: we have never encountered a human or animal that is showing what we would call 'intelligent' behavior that does not have a brain. Moreover, if some organism has some kind of brain damage, we often see the immediate effect of this on their cognitive capacities. So, if the function of the brain is to process information, then all of this can be seen as a kind of confirmation of computationalism. 
It should be obvious that if computationalism is true, then computers can be intelligent, which is the very focus of the First War. Indeed, the First War can largely be rephrased as: Is Computationalism true? To understand and analyze the arguments made within this debate, we need to get clear on exactly what the claim of computationalism is. So, in Chapter 3, we will include a more technical account of what constitutes computations. As it turns out, this technical discussion will unearth some further reasons in favor of computationalism. Chapter 4 will then discuss more on the nature of the computational systems, and the different ways they can be implemented.
In 1936, Alan Turing suggested a mathematical definition of computation that turns out to be a really helpful way to think about computations, and indeed to make the claims of computationalism accordingly quite precise. In fact, even though the concepts of 'computation' and 'computer' had been around before Turing did his important work, Turing's proposal to capture these ideas have shaped our very concepts of computation themselves. Still, before we go into Turing's work, let us first have an informal discussion on the nature of computation.
A computation is a systematic process of symbol-manipulation. An illustrative example might be the addition of two numbers, which is something we learned in elementary school: write the numbers under each other, add the two least significant digits, write down the result and the possible carry, move to the next least significant digits, and repeat until we've exhausted all digits of the original numbers. In this example, we can see two crucial elements of a computation: symbols and systematic processes. Let's consider these in turn.

When doing number addition on paper, what we are manipulating aren't the numbers themselves, but their numerals, which are a representation of the numbers (typically a decimal representation). Indeed, the symbols we are manipulating are 'symbols' exactly because they represent something. And whatever the symbols represent is what the computation is about, or what the computation computes. In the case of the example of adding two numbers, what is being computed is the sum of two numbers.
We can thus define a symbol as anything that takes on the role of representing something. And that literally can be anything. I can, for example, represent the sun and Earth using my lunchbox and my water bottle respectively. Then, when trying to explain the phenomenon of day and night, I can rotate the water bottle. And I can illustrate what a year looks like by having my water bottle circle around my lunch box. 
All of this should be very reminiscent of some of the ideas expressed by functionalism: a symbol is a highly abstract notion, and can thus be multiply realized. Moreover, a symbol is a symbol in virtue of the role it plays in a larger context.

When we are doing addition on paper, we follow a certain set of rules we learned in elementary school. That is, we follow a systematic process for symbol manipulation. Often, this is accomplished through the following of an explicit set of step-by-step instructions, which is typically referred to as an algorithm. That is, we can say that in elementary school we all learned the algorithm for adding two numbers. Now, what makes the procedure of adding numbers systematic, is that it is both effective and deterministic. 
The procedure is said to be effective in the sense that each step is simple enough for us to understand and execute. Indeed, the algorithm has broken down the addition of two numbers into several smaller steps (such as adding two single digits, or writing down a carry) that we know how to accomplish. If the 'algorithm' would have consisted of a single step, stating to "add the two numbers", then for most people this would not be an effective method. 
Also, the algorithm of adding two numbers tells us exactly what we should we doing next. As such, the procedure is deterministic: at each point during the process, it is determined what should happen next. Indeed, if we were to start the same process with the same input, then every step we make will be the same, and hence the end result will be the same as well.
Note that the following of a set of instructions is not always a computation. For example, instructions for putting a piece of furniture together can be seen as a kind of algorithm, but it is not an algorithm for manipulating symbols. Or at least: when we put together a piece of furniture, the individual pieces are typically not treated as symbol that represents anything. A piece of furniture is just that: a piece of furniture. It does not represent anything, and when we are putting it together, we are not trying to figure something out about something. Likewise, a cookbook recipe is a systematic process, but it is not a computation. We only talk about computations when the objects that are being manipulated are used to represent something else, i.e. when what we manipulate are symbols.
Also note that the systematic process we go through is dependent on the nature of these representations. For example, had we used binary representations of the numbers, and had the last digits of the two numbers both have been a 1, then instead of writing down a 2 as we normally do, we write down a 0 and produce a carry. 

A computer is anything that implements or 'goes through' a systematic process of symbol-computation. There are many kinds of computers. Nowadays we think of a 'computer' as an electronic device, but some of the first computers were completely mechanical, using wheels, cogs, pulleys, and levers to manipulate whatever physical objects were used as the representations of whatever the computation was about. Engineers are also looking at optical computers, where light is being used to compute, and quantum computers, where quantum-mechanical states are being used. 
Before we had these modern-day computers, it was actually us humans who were the computers. Indeed, it was only when we figured out how to mechanize and automate the process of computations that we obtained the non-human, 'mechanical' computers that we nowadays associate with the word 'computer'. However, even nowadays, every time we do some number addition on paper, we are the ones doing the computation, and as such can be a 'computer' as well.
When it comes to the modern-day mechanical/electronic computer, we immediately recognize that it is computer programs that take the role of the algorithm. That is, the computer program tells the computer what to do every step of the way. However, it is important to realize that there may not always be an explicit algorithm involved. The very first mechanical computers, for example, did not have any explicit instructions. They were built to do just one or a select few things. Indeed, it was only later that we figured out how to make the computers the kind of programmable computers that we are more familiar with nowadays: machines that can be given different programs to perform different processes of symbol-manipulation. But again, the important point is that there may also be computers that do not have explicit programs: machines that are 'hardwired' to implement a specific computation.
As such, a computer 'executing an algorithm' is actually a bit of an ambiguous notion, for it could mean that the (programmable) computer is using a program to do its task, or it could mean that the (specific-purpose) computer makes no use of any such description or program, but whose behavior could nevertheless be said to be in accordance with a program. 
In the former case we can therefore speak of an explicit program: a set of rules that the computer follows. In the latter case there is only an implicit program: a set of rules that the computer obeys. Of course, as we create a machine of the latter type, we most likely had an algorithm in mind, but we never translated that into a piece of code that the machine can interpret and work with. Rather, we just hardwired the machine to behave in accordance to the algorithm we had in mind.
As an analogy, think of the movement of planets. The laws of Newtonian mechanics describe their behavior and, as such, the planets can be seen to 'obey' those Newtonian laws, and of course whatever other laws of nature there may be. But in doing so, the planets are of course not consulting or following any kind of book of instructions that tells them where they should be going next.

In 1936, Alan Turing wrote a famous paper called 'On Computable Numbers, with an application to the Entscheidungsproblem'.The 'Entscheidungsproblem' (German for 'decision problem') was an important problem in mathematical logic posed by David Hilbert in 1928: given a sentence expressed in first-order logic, is there an algorithm that we can follow in order to decide whether it is a logical theorem or not? In chapter 6 we will actually explain exactly what is meant by this, but for now, the important thing to note is that at the time Turing did his work, it was suspected that there is no such decision procedure. However, how would you prove such a thing? You would need to be able to say that no systematic process of symbol-manipulation would be able to do the task at hand. And, to do that, you would need a general notion of 'computation' that would indeed capture all possible computations. So, this is exactly what Turing set out to do in his paper. 
It was indeed in the process of answering this question about logic and mathematics, that Turing, almost as a by-product, laid out all the underpinnings of theoretical and practical computer science as we know today. Indeed, Turing's invention of the universal machine is what has become our ubiquitously used modern general-purpose computer.
Turing aimed for a mathematical characterization of an 'effective computation' which, as the reader may recall, means that a human being is able to perform the individual steps of symbol-manipulation'. Turing therefore clearly had humans in mind as the 'computer'. However, Turing also recognized that if the individual steps of symbol-manipulation would be small enough, then it should also be possible to automate these steps using a mechanical device, in which case we can talk of a 'machine'. This is why Turing's work is generally taken to be laying out the nature of what we now call Turing Machines. 
Turing's path that led him to his eventual definition is actually similar to what we have done in this chapter so far. Any computer, Turing said, manipulates a bunch of symbols, which we can think of written down on a sheet of paper. Also, the process of symbol-manipulation is a step-by-step, deterministic process. So far so good. OK, but how exactly is it exactly, Turing asked, that the next step of the process determined? Turing concluded that it depends on exactly two things: it depends on what state the computer is in, and what symbols the computer sees in front of it. 
What does Turing mean by 'state'? Going back to the addition of numbers, we may at some point find ourselves in the 'state' of having to add two specific digits, followed by the 'state' of having to write down the result, etc. Now, Turing said, while our natural language description of these states may use words like 'adding two digits' or 'writing down the result', all that is really important is that the computer is able to distinguish between the different states. That is, for all that the computer cares, the different states could simply be numbered 1, 2, 3, etc. and all that the computer needs to know in order to make its next move, is to know whether we're in state 1,2, or whichever other state there may be there. 
A similar story goes for the symbols, Turing argued. That is, while we are used to the decimal representation of numbers, we could of course use any other kind of representational scheme. Again, Turing argued, all that matters is that we are able to distinguish one symbol from a different symbol. Or, a little more technical, if we use a set of symbols s1, s2, s3, etc. then all that should matter to the computer is that the computer be able to recognize that some written symbol is, say, symbol s2, rather than any of the other symbols. In short, the computer simply needs to have an alphabet of symbols that the computer is able to read and write.
How are the symbols organized? Here, Turing makes another important realization:

Computing is normally done by writing certain symbols on paper. We may suppose this paper to be divided into squares like a child’s arithmetic book. In elementary arithmetic the 2-dimensional character of the paper is sometimes used. But such use is always avoidable, and I think it will be agreed that the two-dimensional character of paper is no essential of computation. I assume then that the computation is carried out on one-dimensional paper, on a tape divided into squares. 

Turing here makes the point that whatever we can compute using a two-dimensional (or even higher-dimensional, as when we use multiple sheets of paper) layout for the symbols, we can always simulate using a one-dimensional layout, since we can always 'string' together all the lines and make it into one long string of symbols, possibly using special symbols to demarcate what would normally be line, page, or other breaks. However, since it is not predictable how much 'scratch' work one has to do while performing the computation, Turing decided that we can think of having a tape with an infinite number of squares on it, and that symbols can be placed in each of those squares. Or, maybe more realistically, that one always has the ability extend the tape by adding squares on either direction, so that one is guaranteed of any amount of 'working space' as needed.
How many states and symbols can there be? Turing said that there could be any number, as long as it is finite. Remember that Turing wanted to ensure that humans would be able to perform the computation, and thus Turing argued for the finiteness of states by saying that the human mind and human memory was limited. Also, assuming a finite size of the squares on the tape in which to place our symbols, there can only be finitely many different symbols, or else the differences between the symbols would become infinitely small, and we humans would no longer be able to recognize and distinguish the one symbol from the next. 
Finally, as a computer we must be able to go to any arbitrary place in this one long string of symbols. But for that, Turing argued, all that is required is an ability to go either left or right one square relative from the square one is currently looking at: wherever one wants to go, eventually should be able to get there. Thus, Turing imagined that one would have a 'read/write' head that at any particular point in time would be 'at' some particular square on the tape, and that this 'head' can move left or right, one square at a time.
And there you have it! These are all the components that make up what we now call a Turing machine: A machine that has a read/write head to read and write symbols that are printed in the squares of a non-ending piece of tape. The machine has some finite number of internal states, there is finite number of symbols being used, and the machine has the ability to move left and right one square from the square that the read/write head is currently at. The machine is 'rigged up' (i.e. has a certain causal, functional, organization) so that depending on whatever internal state it is in, and whatever symbol is in the 'current' square, the machine will write down some new symbol in that square, move left or right, and go to some new internal state, after which the cycle repeats, or the process comes to a halt.

With the definition of Turing-machines in place, we can now ask what kind of things we can compute with them. Typically, we talk about this in terms of functions. That is, we say that Turing-machine M computes some function f: X → Y if and only if there is some reasonable representation convention that represents objects x from the domain X as [x], and that represents objects y from the co-domain Y as [y], such that whenever f(x)=y, the Turing-machine is able to transform [x] into [y]. The [x] and the [y] are usually defined as tape configurations. That is, representing an object x as tape configuration [x] means that we specify what the input tape looks like, and where the 'head' of the machine is specified at some particular square of the tape.
As an example, suppose that we want to use a Turing machine to add two natural numbers. That is, we want a Turing machine that computes the function fADD: ℕ × ℕ → ℕ defined as fADD(m,n) = m + n for any m, n  ℕ where ℕ is the set of natural numbers.
To compute fADD, we first have to somehow represent m and n on the input tape. Now, we can't use a separate symbol for each natural number, since we are only allowed finitely many symbols. We could of course use the decimal representation of m and n, in which case we only need the ten digits we all know, but this would still entail a sizable number of possible state-symbol pairs, and to keep things easy, we will do the following. To represent a number n on a tape, we just put a series of n consecutive 1's on the tape, and we'll assume that all other squares on the tape are 0's (or blanks). This, together with the location of the 'head' being at the leftmost 1, defines a tape configuration we'll write as [n]. 
To represent m and n both at once, we simply put m 1's on the tape, followed by a 0, followed by n 1's. Again, the rest of the tape will be blanks. Again, with the 'head' at the leftmost 1, we will write this tape configuration as [m,n]. Given this representation convention, the task of the Turing machine is to transform any input tape configuration [m,n] into output tape configuration [m+n] for any numbers m and n.
It turns out that a Turing machine can be specified that does exactly this desired transformation. All the machine has to do is to move right until it finds the blank between the two sequences of 1's, write a 1 at this spot, move right again until it finds the end of the sequence of 1's, erase the last 1, and move left until it is at the leftmost 1. However, other solutions are possible as well. For example, you can first erase the initial 1, move right until the blank is found, write a 1 at this spot, and then move back to the leftmost 1. The first strategy is depicted in Figure 3.1.
 
In this diagram, the nodes represent the possible states the machine can be in. The machine starts in state q0. Since the first thing we want the machine to do is to move right until it sees the 0 in the middle, we have it do the following. As long as you see a 1, 'replace' it with a 1, move one square to the right, and stay in state q0. This is all captured by the loop that goes from q0 to q0: "If you are in state q0 and you see a 1, then write a 1, move right, and go to state q0". Indeed, notice how every transition has exactly this same 5-tuple form: "If you are in state [some state] and you see a [some symbol], then write a [some symbol], move [left, right, or stay], and go to [some state]". 
Now, this diagram only specifies the desired workings of a machine. That is, it specifies an algorithm that, when executed, accomplishes the desired transformation. To obtain an actual computation, we therefore still need to create a physical machine that has the right causal and functional organization to actually behave in accordance with this specification. Such a machine can be said to implement or realize the algorithm. However, what such a machine actually look like is not our concern, just as computer scientists in general only study algorithms, and leave the problem of how to actually create physical devices to execute those algorithms to the computer engineers. 
Indeed, from a pure computational point of view, we do not need to know how exactly the adding machine is implemented in order to observe its behavior in terms of symbol manipulations. For example, let's suppose the machine is presented with the input tape as given in the top of Figure 3.2. The triangle indicates the square that the read/write is 'at', and hence what symbol the machine is looking at. As per the convention we laid out, we start at the leftmost 1. Now, the reader can easily play the part of the computer, and follow the instructions as given by Figure 3.1, going from state to state, and changing symbols on the tape and moving the read/write head as instructed by the instructions one encounters.
 
If you do this correctly, you should find yourself in state q3, with no further instruction to do. At that time, the machine is said to have halted, and we should be able to look at the tape to see what the machines has produced. If all went well, you should have gone through each of the transformations as shown in Figure 3.2 and with the last step resulting in tape configuration [4]. The machine can thus be said to have figured out that 2+2=4. Indeed, a moment's reflection on the nature of the program should convince the reader that for any input [m,n], the machine will halt, leaving out tape [m+n], exactly as desired. As such, we can declare this Turing-machine to compute the addition function fADD.
Notice that only a few states and instructions were needed to compute the addition function. Of course, this is mainly due to the way we choose to represent the input numbers and the output number. Again, if we would have used decimal representations, then the machine that would perform the desired transformations would have been quite a bit more complex. Hence, the chosen representation enormously affects the nature of the Turing machine that has to perform the required transformation. Indeed, the slogan 'defining the problem is half the solution' is well known for computer programmers.

 
In general, we can consider any Turing-machine M as a kind of 'black box' that transforms any given input tape configuration Tin into some output tape configuration Tout, as depicted in Figure 3.3. Assuming the tape configurations are used to represent something (in the example, we used them to represent numbers, but we can use them to represent anything we want), the Turing-machine can thus be said to compute something about whatever is being represented.
We said before that we weren't really interested in how exactly one would create a machine to execute the algorithm as laid out by the diagram in Figure 3.1. However, earlier we observed that there are two fundamentally distinct ways to actually do this, leading to the important difference between 'rule-following' and 'rule-obeying'. 
That is, one way to execute the algorithm as specified by Figure 3.1 is to create a machine that is hard-wired to behave in accordance to that algorithm. This would therefore be a specific-purpose machine: it can add numbers ... but that's all it is designed to do. This is in fact what we do when we interpret Figure 3.1 as a 'machine': we see it as something that 'obeys' the instructions of manipulating symbols as specified. Indeed, with such a machine, there are no explicit instructions to be follows: there is only the behavior leading to the symbols to be processed, though we can accurately describe its behavior with the algorithm, just as we can accurately describe the movement of planets using physical formulas. As such, we can say that the machine instantiates the algorithm.
On the other hand, when the reader was asked to play the role of the computer, the reader took Figure 3.1 to be a 'program', and 'followed' the instructions for manipulating the symbols as specified. Indeed, had a different set of instructions been given to the reader, the reader would have acted on those different instructions. As such, the reader was, in a way, a programmable computer, taking the instructions as the program to execute. However, we can also have machines play this role: a programmable machine.
Now, the idea of a programmable machine other than a human had actually been around for a while before Alan Turing did his work. This should not be a real surprise: mechanical machines with certain 'settings' that can make a machine do one thing rather than some other thing had been around for centuries. The Jacquard Loom took this one step further by using punch cards as an explicit set of instructions for a mechanical loom to weave any kind of pattern as desired. Charles Babbage conceived, though never got to completely build, his Analytic Engine, which was likewise operated by punch cards, and was designed to perform symbol manipulations of various kinds. As such, his Analytical Engine was not just a programmable machine, but a programmable mechanical computer. 
However, it was Alan Turing who found out that there could be such a thing as a universal computer: a programmable computer that, given the right program, can act like any other computer. To be precise, what Turing showed is that there is a Turing-machine U that can be given a description (i.e. a program) of any Turing-machine, and follow it so that its symbol-manipulation behavior mirrors that of the Turing machine whose program is given to it. It is actually not hard to show that this is possible. For example, we can take the specification of the workings of the Adding machine from Figure 3.1, but instead of using a 'flow diagram', we represent it as a set of 5-tuples:

< q0, 1, 1, R, q0 >
< q0, 0, 1, L, q1 >
< q1, 1, 1, L, q1 >
< q1, 0, 0, R, q2 >
< q2, 1, 0, R, q3 >
Now, if we simply put these 5-tuples next to each other, then we obtain a long string of symbols that we can put onto the tape of a Turing machine. As such, the instructions have become a program that has become the input for the Universal Turing-machine. 
Figure 3.5 shows what such a tape would look like. Note that by putting each symbol into its own square, we don't even need any commas or brackets. In fact, even a state like q0 has been represented simply by a 0: all that the Universal Turing-machine needs to do is be able to distinguish between the different states and symbols, and as long as we follow the prescribed order of the 5-tuples, the Universal Turing-Machine will be able to understand what is supposed to be a state, and what is supposed to represent a symbol. 
 
Note that this input tape also contains the input '2+2' that we would like the adding machine to work on. Indeed, in general, a Universal Turing machine U gets as input a tape configuration [M,T], consisting of the specification of some machine M, together with the specification of some input configuration T. In short, U is given a program, and runs that program on some input. As such, the Universal Turing-Machines simulates or emulates the workings of the adding machine working on some input. Indeed, the very distinction between hardware and software is effectively the distinction between the universal computer and the programs that it can execute. Also, when a universal machine runs a specific program, it behaves in accordance to, and thus effectively becomes, the machine that it is simulating. Thus we obtain what is called a virtual machine.
 
You might think that a Universal Turing-machine is a very special machine, and of course theoretically it is, but as it turns out, our laptops, desktops, smartphones, and most other computational devices around us our all Universal machines: they all have the ability to 'execute' computer programs (or 'apps'), and thereby 'emulate' the behavior of specific machines, thus being able to take on the role of calculator, word editing program, web browser, driving navigator, and all the other programs we know.
A machine, when given some input tape configuration, does not always halt. For example, think of a machine that is given an 'empty' input tape (all the squares are a 0), starts in state 1. and whose only instruction is "If you are in state 1 and see a 0, write a 1, move to the right, and go to state 1". Such a machine would forever move to the right, replacing every 0 with a 1, and would never come to a halt. Notice that in such a case, there is no output tape configuration Tout defined.
When working with an actual computer, it is often annoying for the computer to not halt: rather than accomplishing (i.e. finishing) some kind of task, the computer will get stuck in an infinite loop, resulting in what we perceive as a 'freeze' or 'crash'. Indeed, unlike the mathematically defined Turing machines that have an infinite tape to work on, actual computers have only finite memory, and can therefore run out of memory, requiring the user to terminate the machine and restart or reboot the machine. So, it would be very nice if, before we were to actually have a machine run on some input, we could solve what is known as the Halting Problem: can we decide whether some machine M, when provided some input T, is going to halt or not halt? 
Now, you might think that the Universal Turing-machine would be able to solve exactly this problem: The Universal Turing machine U takes as input [M,T], and then, by its very nature, simply simulates how machine M manipulates input tape T, and thus can find out whether M with input T will eventually halt or not. However, this strategy does not work: if machine M with input tape T does never halt, then of course U's simulation of M working on T will not halt either. So, the universal Turing machine in fact does not solve this problem: it can find that machine M with input T halts, but it cannot find that M with T does not halt, because in order to 'find' that some machine does not halt, it would need to halt in order to let us know that, and simulating a non-halting machine means that it won't halt itself either.
To think about this a little differently: imagine a Universal Turing-machine that has simulated some other machine for a trillion steps, and has not halted yet. This might suggest that the machine that is being simulated does not halt, but of course you don't know this: maybe it will halt after another 10 more steps. So, a Universal Turing machine will never halt when it simulates a non-halting machine, but at no point will it be able to determine that it never halts.
So, we cannot just run or simulate a machine to know whether it eventually halts or not: if it doesn't halt, we need to somehow figure that out. That is, we need to find some way to reason about the machine and its behavior. However, Turing showed that any such attempt to figure it out is doomed to fail. To be precise, while for some specific machines and inputs we may be able to tell whether it halts or not, there is no Turing machine H that can take in any machine-input pair [M,T], and figure out whether M with input T will halt or not.
We are going to prove that there is indeed no such machine H, by showing that if there were such a machine H, we would obtain a logical contradiction. So, suppose there is a machine H that can take in [M,T], and figure out whether machine M with input tape T will eventually halt or not. Of course, H must have some way of indicating this on its output tape, so let's say that H outputs [1] to indicate that machine M with input T will halt, and outputs [0] if machine M with input T does not halt. In all cases, H is supposed to halt and leave the correct answer, and thus the assumption is that we have a machine H whose input-output behavior is specified as in Figure 3.
Now, if such an H exists, then we can easily create a slightly more complicated machine, which we will call Q, on the basis of H. Q is depicted in Figure 3.8 The figure shows that H is being used as a subroutine of Q, to which two other subroutines are added: the Copy subroutine takes whatever is provided on its input, and produces two copies of it (separated by a 0) on its output (thus, for example, the Copy machine would transform [n] into [n,n]), while the Trailer machine is set up in such a way so that if gets as input [1], then it will get stuck in some infinite loop (and thus not halt), but if its input is [0], then the Trailer machine will immediately halt.

Both the Copy and the Trailer routine are easily shown to be Turing-computable. So, if H exists, then Q exists as well. However, being a Turing-machine itself, we can describe Q's working as a Turing-machine program on an input tape. And now consider what happens when we provide machine Q with its very own program [Q]. That is, what will happen if we give input tape [Q] to machine Q? Well, the way Q is specified, Q will first use the Copy subroutine to create a duplicate of that description on the input tape, i.e. Q will transform [Q] into [Q,Q]. This is now passed to machine H, which treats the first part of the input as the description of a machine (i.e. as Q), and the second part as a description of the input tape provided to Q (i.e. [Q]), and H will thus decide whether machine Q with input [Q] will halt or not.
Now, suppose Q with input [Q] halts. Then H will figure this out, and output [1]. But now Q's Trailer routine kicks in, and Q will go into an infinite loop. In other words, if Q with input [Q] halts, then it doesn't halt. OK, but what if Q with input [Q] does not halt? Then H will output [0], in which case the Trailer routine immediately stops, and thus Q with input [Q] ends up halting. So, if Q with input [Q] does not halt, then Q with input [Q] does halt. 
So, we find that Q with input [Q] halts if and only if Q with input [Q] does not halt. But that is a logical contradiction. So, something is impossible here, and again, since the Copy and Trailer routine are perfectly possible, the conclusion must be that H’s input-output functionality must be impossible to achieve on a Turing-machine. So, we see that the Halting Problem is not solvable by a Turing machine.
One might think that it shouldn't be all that hard to figure out whether some machine halts on some given input, but as an example as to how hard this can be, consider the Turing-machine from Figure 3.9. This machine looks pretty simple: it only has 5 states and manipulates only 2 symbols. Now, let's give it a very simple input as well: an all empty (all 0) tape. Will it halt or not? If you, the reader, can figure that out, you would be the first: the behavior of this machine is so chaotic that human researchers still don't know whether it will eventually halt or not after several decades of research. And again, just running the machine, and noticing that it still hasn't halted after a long, long time, does not mean that it is a non-halter: there is a Turing-machine with 6 states and 2 symbols that is known to halt after taking more than 1036534 steps!

As stated before, Turing laid out his definition of what we now call 'Turing-computation' to try and capture any form of effective computation. That is, Turing took the informal notion or concept of 'effective computation', and tried to capture it with his mathematical definition of Turing-computation. If Turing did this correctly, then whatever is computable through any kind of effective method, is computable by a Turing-machine. We can't really prove this claim mathematically though, since we can't prove whether Turing's mathematical definition captures our informal concept of 'computation'. Hence, we call this Turing's Thesis.
Now, a year before Alan Turing's work on Turing-computation, Alonzo Church had defined a notion of 'effective calculability' that, like Turing's proposal, was meant to provide a definition of what could be 'calculated' through any effective method. Church believed that every function defined over the natural numbers whose function value could be figured out using any 'effective' method had to be a function that can be defined as the successive application of many simpler functions (such as basic constant functions or addition functions), which he defined as 'effectively calculable' functions. Thus, although once again not mathematically provable, Church's Thesis was that every function that we could 'effectively' figure out is an 'effective calculable' function.
Interestingly, Turing was able to prove that his mathematical definition of Turing-computation was equivalent to Church's mathematical definition of effective calculability. That is, he showed that a function from natural numbers to natural numbers is Turing-computable if and only if it is effectively calculable. The very fact that these two quite different mathematical definitions for 'effectively figuring things out' turned out to be able to figure out the exact same things suggests that there is a kind of natural class of things that are 'figure-out-able'. Moreover, as the Halting Problem shows, this is clearly not just everything, thus strongly suggesting that there is a kind of 'natural' limit to effective procedures, which is often referred to as the Turing-limit. 
Now, it is generally felt that Turing's proposal does a better job capturing the very nature of symbol manipulation more directly than did Church's. For one, Church's definition was specifically about functions defined over natural numbers, while Turing's definition contemplated any process of symbol manipulation, whether those symbols are used to represent numbers or other things. Second, following Turing's line of thought that led to his definition of Turing-machines is inherently persuasive that this should indeed cover any 'effective' kind of symbol-manipulation, whereas one does not obtain such immediate intuitions following Church's proposal. Simply put: there is a stronger intuition supporting the truth of Turing's Thesis than supporting the truth of Church's Thesis. Still, Church did his work a year before Turing, was making a conceptually very similar claim that mathematically turned out to be an equivalent claim. Thus, both researchers were given credit for their work, and their theses were combined into what is now known as the Church-Turing Thesis.

As mentioned at the start of section 3.5, Turing developed his notion of Turing-computability in order to provide an answer to the 'Entscheidingsproblem' or Decision Problem: given any logical argument, can we figure out whether it is valid or not? More details on the relevance of this problem will be provided in chapter 6, but for now, it suffices to quickly discuss how Turing argued that the Decision Problem is not solvable. 
First, Turing showed how any pair of Turing-machine and tape configuration can be expressed as logical statements, and that there is also a logic statement (the 'Halting statement') claiming that there is some point in time at which the machine has halted. Turing also showed that the Halting statement would logically follow from the statements describing machine M and some input tape configuration T if and only if M, when starting on T, will halt at some point. However, that means that if we can figure out whether some statement is a logical consequence of some set of other statements, then we can solve the Halting Problem, for in order to solve the Halting Problem, we would simply describe the machine and its input using logic, and ask whether the Halting statement is a logical consequence or not. But since it was already shown that the Halting problem is not solvable, it follows that the Decision Problem is not solvable either: there is no effective 'decision procedure' for logical consequence. Together with the Church-Turing Thesis, which says that any solvable problem is Turing-solvable, we thus obtain that the Decision Problem is unsolvable, period.
Since the work by Church and Turing, many other forms of computation have been proposed, such as register machines, recursive functions, and others. All, however, have been proven to be of equivalent power, in the sense that the class of computable functions for each method was exactly the same. These results thus provide further evidence for the claim that there is a sharply defined set of things that can be figured out through any 'effective' method, and hence for the Church-Turing thesis.
Some researchers have proposed models of hypercomputation, which are forms of computation that are more powerful than Turing-computation. Typical models of hypercomputation appeal to some form of infinity, whether it is able to discern the infinite decimal expansion of real numbers, or be able to perform an infinite number of operations in some finite amount of time. For example, an Accelerated Turing-Machine is a Turing-machine that can perform each operation in half the time it took to perform its previous operation. Mathematically, this would mean that if the first operation took 1 second, then after two seconds, this machine would have performed an infinite number of operations. As such, an accelerated Turing-machine could solve the Halting Problem: simply simulate the machine and input tape to be considered just as a Universal Turing-machine would, and report if you ever find the machine to halt. If after two seconds it was never reported that the machine halts, then we know the machine never halted! 
Now, there is some debate as to whether such a machine is physically feasible or, for that matter, even logically coherent. Indeed, in order to see the very logical problems that this machine presents, consider that an accelerated Turing machine would need to go through an infinite number of states ... and somehow be able to a state where it has gone through each and every one of an infinite number of states. But how is that possible? If there are an infinite number of states, and you go from one to the next, then you can only go from a state where you have gone through a finite number of states to a new state where you still have gone through a finite number of states only. And saying that "Just wait two seconds!" is pure bluff: yes, we know that in our world two seconds can pass, but does that mean that we have gone through an infinite number of distinct points in time? 
Indeed, Zeno argued over two thousand years ago that the very fact that time flows in our world means that in our world time is not infinitely divisible. In other words, while it is certainly logically and mathematically possible for a space-time topology to be infinitely divisible, it does not seem logically possible to actually traverse such a landscape, going from one point to the next or, as in the case of an accelerated Turing-machine, from one state to the next. 
Of course, Zeno's paradoxes are still being discussed today, and maybe there are ways to resolve the logical paradoxes that hyper computation presents itself. However, what does seem to be clear, is that none of these proposed forms of computation are 'effective'. That is, none of these forms of computation provide us with algorithms of symbol-manipulation that a human can follow in order to figure something out, because humans cannot write down infinitely many symbols or perform infinitely many steps.
Indeed, one may wonder: in what sense would these hypercomputations even be 'computations' then? Well, remember the distinction between rule-following and rule-obeying. Yes, it is true that a human cannot follow any algorithm that exploits the infinite, and so the algorithm would not be effective. However, it might still be possible for some physical system to behave in such a way that it obeys some step-by-step algorithm that can be used to process information. Indeed, maybe the human brain is exactly such a physical system that realizes hyper-computational algorithms going beyond the Turing-limit. 
On the other hand, if the physical world around us is of a 'finitistic' nature, then we have excellent reasons to believe that there is no such thing as hypercomputation, and that all computations are limited by the Turing-limit. In fact, given the highly elementary nature of the operations involved, we would have reason to believe that all physical processes are within the Turing-limit: that no information-processing can be done by any physical system beyond the Turing limit. This latter claim is sometimes referred to as the Physical Church-Turing Thesis.
We saw earlier that the same function can be computed by different Turing-machines. However, it is also true that the same Turing-machine can compute different functions. To see this, go back to the example of adding numbers. In the example, we used the convention of representing a number n with a block of n successive 1s on the tape. However, this means that the number 0 gets represented by a block of 0 successive 1s, and thus effectively by nothing at all. Indeed, using the typical convention of having the read-write head start at the left-most 1 on the input tape, there would be no difference between, say, [0,3], and [3,0]. But, if we are asked to compute the function xy, then this should make a big difference, since 03 is not the same as 30. To cope with this issue, a typical strategy is therefore to represent the number n with a block of n+1 successive 1's, so that the number 0 gets represented by a single However, notice what happens if we run the Turing-machine from Figure 3.1 on input tapes following this modified convention. When given an input tape that has n successive 1's followed by a 0, followed by m successive 1's, it will, as before, halt with an output tape that has a single block of n+m successive 1's. But with the new representation convention, a block of n successive 1's represents the number n-1. Hence, interpreting the input and output tape configurations as numbers, we see that the Turing-machine the two numbers n-1 and m-1, the machine outputs the number n+m-1. But that is a number that does not equal (n-1)+(m-1). Indeed, a moment's thought will reveal that given the new representation convention, the machine can be seen to compute the function fADD+1(n,m) = n+m+1. So yes, we can have the same machine compute different functions, all depending on what the symbols are supposed to represent. 
The result that the same computational process can compute different things is at first sight a little paradoxical: if the computation is the same, then how can it be computing different things? To resolve this paradox, it is useful to make a distinction between what we might want to call a process of syntactic computation, which is a process of transforming input symbols to output symbols, and a process of semantic computation, which is a process of computing some function f: X → Y with the use of a syntactic computation together with a convention of how to the objects x  X and y  Y are represented by the input and output symbols. As such, syntactic computations are a strict part of semantic computations, as depicted in Figure 3.10. Indeed, with this distinction, the earlier paradox is now easily resolved: The same syntactic computation can be part of different semantic computations. 
 
The distinction between syntactic computation and semantic computation has a number of other important consequences. For example, go back to the Halting Problem. We claimed to have proven that a Turing-machine cannot solve the Halting Problem. What exactly do we mean by this? Well, given that the Halting Machine would have to figure out something about some other machine and their input, we clearly took this result as a limitation on semantic computation. 
However, if you go back to the proof, you will find that what we really proved, was merely a limitation on the syntactic computational abilities of Turing-machines. That is, we showed that a contradiction would be reached if the Halting machine would be able to transform input tape configuration [M,T] into either [0] or [1] depending on whether or not M with input T would halt or not respectively. Thus, all we really showed is that no machine would be able to compute the Halting function fH: M × T → { 'Halts', 'Does not halt' } using this particular convention of representing machines, input tape configurations, and the eventual determination as to whether the machine halts or not. 
Put differently: all we really did was to prove a limitation to the syntactical computational abilities of Turing-machines. So, an obvious question remains: could a Turing-machine have computed the Halting function using some other representation convention? Indeed, could there be some way to use a Turing-machine to semantically compute the Halting function after all? 
This last question is much more difficult to answer. Indeed, without a mathematical definition of semantic computation, we really cannot prove that there is no semantic computation of the Halting function. Of course, one way to try and define the semantic computation of a function using a Turing-machine is to do the following:
A function f: X → Y is (semantically) Turing-computable if and only if there exists some encoding function c: X → TC, some Turing-machine M:TC → TC, and some decoding function d: TC →Y such that for any x  X, we have that d(M(c(x)))=f(x).
However, there are some serious problems with this proposal. For example, there is nothing in the definition that prevents one from simply defining the encoding function c in such a way that the answer y=f(x) is immediately represented on the input tape configuration. That is, we could basically set c(x)=[f(x)], and since the decoding function will decode this tape configuration as the desired output f(i), it follows that every function becomes trivially semantically computable, using a Turing-machine that does absolutely nothing at all! 
To give a concrete example, under this definition of semantic computation, the Halting function becomes trivially computable, since we simply define c(M,T)=[1] if machine M with input T halts, and c(M,T)=[Of course, having a representation convention that already represents the answer to whatever needs to be figured out on the input to be given to a machine would be a completely 'unreasonable' representation convention. Indeed, while it is one thing to define a mapping between objects and the symbols representing those objects, this mapping should of course be something that we are able to 'perform'. That is, there should be some 'effective' way to figure out how to represent objects in accordance to some representation convention. But this is clearly not something that is true for the encoding convention we defined in the previous paragraph: given some machine M and input T, I cannot, in general, figure out whether M with T will halt or not, and hence I do not know whether to 'represent' that input with [1] or [0].
To include the desired output as part of the input is called 'bramming'. Another example of 'bramming' is to exploit the decoding function. For example, using the 'normal encoding c(M,T)=[M,T], we can define decoding d([M,T])=1 if M with T halts, and d([M,T])=0 if M with T does not halt. And there are other 'tricks' yet, all of which Jack Copeland calls 'deviant codings' for the reasons as stated above. So, the question becomes: how can we avoid such deviant codings in our definition of semantic computation? In other words: how can we ensure that the mapping between objects and the representations thereof is an 'effective' encoding? And, if we want to preserve the result that the Halting function is not computable, how can we show that there is no 'effective' coding scheme that a Turing-machine could use to compute the Halting function with?
Unfortunately, since the objects that the computation has to be about can be anything, including abstract thoughts and ideas, it does not seem like there is a better answer than to simply insist that the codings be 'effective', i.e. that a human is able to effectively represent the objects under consideration. However, if a human can 'effectively' perform the coding and decoding, then a human should be able to 'effectively' transform one 'effective' coding into another. Thus, invoking the Church-Turing Thesis, it is therefore likely that all 'effective' coding can be transformed into each other by a Turing-machine. So, once we have proven the Halting function to be Turing-incomputable using one effective encoding, which is something we did, then we can use the Church-Turing Thesis to argue that the Halting function is not Turing-computable using any 'effective' coding. And, applying Church-Turing Thesis again, we have thus made it highly plausible that the Halting problem is not solvable using any 'effective' method, period.
We are now (finally!) in a position to go back to the view of computationalism. First of all, here are some other interesting facts that we know to be true about Turing-machines that seem to fit, if not support, the thesis of computationalism:
Many functions are Turing-computable. 
Addition is of course a fairly simple function, but it turns out that Turing-machines can compute many more, and much more complicated, functions as well. Indeed, in line with the Church-Turing thesis, it can be shown that whatever the modern digital computer can compute, a Turing-machine can compute as well.
This is relevant to computationalism, since the human mind can perform all kinds of feats. So, if computationalism wants to be true, then it had better be the case that lots of things can be accomplished using computations.
A function can be computed in many different ways. 
In the example, we saw that there are at least two different ways to compute addition. This turns out to be true in general: If there is one Turing-machine that computes some function f, then there will always be other machines that compute f. In fact, it is easily shown that if f is computable at all, then there are an infinite number of Turing-machines that compute it.
This is relevant to computationalism in that it extends the notion of multiple realizability. That is, while the multiple realizability of computations is typically understood as the computation being realizable by different physical organizations (e.g. different physical materials), we can now extend this notion to the abstract functional organizations as well: that is, different computational organizations can accomplish the same eventual functionality, and presumably it is at this higher abstract level that we would encounter the 'level' of cognition. Crudely put: computationalism is functionalism on steroids. But more to the point: two different humans need not go through the exact same processes in order to have the same cognitive capacities. 
You only need 2 symbols (a binary alphabet). 
It can be easily shown that if there is a Turing-machine that computes f, then there is also a Turing-machine that computes f using only two symbols.
This finding is important for computationalism in the following way. Many computationalists believe that the brain implements the computational algorithms underlying cognition: that the brain is a computer of sorts. However, notice that while the brain's architecture is incredibly complicated, the individual neurons that make up the brain all seem to follow a fairly straightforward pattern: fire as a function of the 'incoming' neurons' firing. As such, all the neurons seem to accomplish a fairly 'simple' task, and this task is basically the same for all neurons. Indeed, looking at the brain at the level of these neural firings, everything pretty much looks the same. How then, a common objection goes, can it be that this neural activity would be responsible for, say, seeing a tomato in front of me, while that neural activity is for retrieving some childhood memory?
Well, as the theory of Turing-computation demonstrates, if you combine enough simple mechanisms, even if this is merely the repeated manipulation of a bunch of 0's and 1's, you can obtain very complicated behavior. And those 0's and 1's can really be about anything, depending on how they are being used. Now, computability theory shows that you do need to go through many of these simple operations, but the brain performs lots of 'simple' operations as well. Indeed, if the brain would only go through few neural firings, then computability theory would be at a loss as to how the brain would be able to do any interesting kind of information-processing. So, the fact that the brain does in fact perform lots of these operations is compatible with, if not confirming evidence for, computationalism.
Remember from section 1.2.3 that Turing pointed out the important distinction between different levels of analysis: the level of mechanism or functioning is not at all the same level as the level of behavior or interpretation. The simple actions that happen at the mechanical level may well look like 'unintelligent donkeywork', but complicated, interesting, and often surprising behavior can arise as a result of that at the level of interpretation.
As pointed out several times now, it can be hard to pinpoint where our mind, cognition, or intelligence is located. Some will say that it is 'obviously' somewhere in our head or brain, but that the difficulty of pinpointing it is due to the fact that it is an abstraction, much as the U.S. economy is an abstraction. The view of Embedded Cognition, however, proposes a different answer. While agreeing that mind and cognition are abstract entities, and while also agreeing that the brain plays a very important role in cognition, a proponent of Embedded Cognition states that cognition is not always to be found in the head or the brain, even as an abstraction. Rather, a proponent of Embedded Cognition will say, our mental capacities are, at least sometimes, only to be found or explained when we take into account the interactions we have with our environment.
Now, claiming that one cannot provide a full account of one's cognitive powers without taking into account the environment is a claim we already encountered when discussing Functionalism. That is, according to Functionalism, it is the functioning of the brain relative to its environment, that allows the brain to cognize about that environment. Thus, for example, it is only because light bounces off of external objects that enters my eyeballs and gives rise to brain activity that presumably gives me the ability to see those objects: take the objects away, and I would no longer see them, similarly how a car that is spinning its wheels very fast would not actually be fast when put into the vacuum of outer space.
OK, but all of that sounds rather trivial. Sure, in order to tell the full story of cognition, in particular concerning perception and action, we need to see how the brain is connected up to our sensory-motor systems, and how those systems interact with their environment. But how important is it really to point that all out? How large of a role will those interactions play in the explanation as a whole? When it comes to the fastness of the car, pointing out that the car actually needs to be put on a road in order to be fast seems a rather trivial point. And indeed, the 'core' of the explanation as to why the car is fast would surely be with the nature of its engine. Likewise, when it comes to cognition, isn't it intuitively the brain that will account for 99 percent of the explanation as to why we have the cognitive abilities we do, even if we are talking about something like being able to navigate our environment?
It is exactly at this point that Embedded Cognition will part with some of our intuitions about the role of the brain and its connection to mind, cognition, and intelligence. Embedded Cognition will state that the role of our sensory-motor system, and our interactions with our physical environment, is far from trivial when it comes to explaining cognition. Indeed, the view of Embedded Cognition is a good bit more radical than Functionalism. To get a sense of what Embedded Cognition is really claiming, then, let us start with a simple example: catching a baseball.

How does one catch a baseball when standing in the outfield? 
The 'naive' view, which has been dubbed the 'sense-plan-act' model, is that once we see the batter hit the ball and it starts going up into the air, our brain will compute the trajectory of the ball, and figure out where and when it is going to land. Some people might even imagine that the brain solves some system of equations for Newtonian mechanics to do all this. Anyway, however the brain figures out the whole trajectory, it is assumed that it does, and that once that is completed, the brain will come up with a plan for how to move our limbs in just the right kind of sequence so that we can be at that very anticipated spot at the right time. Finally, we put this plan into action by activating the relevant motor control mechanisms.
However, there are serious problems with this view. First of all, the computations that are being made to figure out the trajectory of the ball and, subsequently, the trajectory of our whole body, would have to make all kinds of assumptions about the world that may not hold true in reality. For example, the brain would need to take into account initial velocity, angle, and distance, not to mention wind velocity, or the spinning of the ball. And it is unreasonable to assume that the brain would know all these factors in high enough detail so as to make an accurate enough calculation, no matter how good the perceptual system is. 
In fact, the world is a dynamic place, and changes all the time. For example, a gust of wind could change the ball's trajectory, and any kind of unevenness of the ground has an effect on the movement of my body. Indeed, a moment's reflection should quickly convince someone that the sense-plan-act approach is doomed to fail in a situation like this, which is something the early roboticists quickly found out as well.
The view of Embedded Cognition therefore proposes a radically different way to handle something like this: instead of trying to figure out everything, and come up with the whole plan, it is much better to attack and solve this problem bit by bit through interactions with the environment. 
In particular, suppose that in your field of vision the ball is moving to the left. How should you respond? By moving to the left of course! How far, and for how long? Well, there is really no need to figure that out. Just start moving to the left, and see what happens. If you see the ball still moving to the left, keep moving to the left yourself. If it starts moving to the right, though, you have gone too far to the left, and you will have to go back to the right a bit. 
Of course, that works well for going left and right, but how does one make sure that one is at the right distance, i.e. that the ball isn’t going to drop in front of you, or behind you? As it turns out, there is a simple strategy we can follow here as well: if in the field of vision, the ball rises at a constant rate, you are in the right spot. Using some physics and mathematics, you can show that if the ball is going to land behind you, then in the projected field of vision it will accelerate, while if it is going to land in front of you, then in the projected field it will decelerate. But of course, your brain does not need to figure all that math and physics to know all this ... it just needs to be able to detect any such acceleration and deceleration, and drive the motor system to adjust accordingly. 
In other words: in order to catch the ball, one just needs to keep looking at the ball, and make small actions accordingly. There is no need for complicated calculations and to figure everything out beforehand. Indeed, instead of complicated, expensive, and error-prone calculations, the brain merely has to do just enough work to figure out what small adjustments need to be made in response to changes in the environment. Keep following this strategy, and a solution will automatically emerge. Moreover, by repeated interactions with the environment, any noise or uncertainty can be dealt with in time, making for a much more robust approach to catching the baseball than trying to figure it all out beforehand.
However, the most important point of this example is that on this view, solving the cognitive task of catching a baseball reduces to more than just what happens between our ears. That is, Embedded Cognition sees interactions with the environment as part and parcel of cognitive activity. As opposed to the sense-plan-act model, which we might also dub the 'sense-cognize-act' model, cognition isn't always to be found purely in the brain, but sometimes involves many perception-action cycles, with the environment as an important component to guide the cognitive activity. That is, repeated cycles of sense-process-act underlie cognition. As such, Embedded Cognition not only proposes a radically different brain architecture, but provides a different account as to what constitutes cognition that opposes the classical views that view cognition as 'all in the head'.

Of course, as far as 'cognition' goes, one may not find this example of catching a baseball to be all that relevant to things like reasoning or decision-making. Indeed, is this really about cognition at all? Well, as another example of Embedded Cognition, consider the fact that we often leave notes to ourselves to remember things. According to Embedded Cognition, such 'external memory' is as much a part of our memory as the kind of 'internal memory' that, presumably, our brain supports. For example, when explaining why it is that I came to the meeting, we can say that I remembered that there was a meeting, and it really doesn't matter whether that memory was internal or external.
Another interesting example is solving a Sudoku puzzle. If one were to build a computer program to do this, one may be quickly drawn to a kind of classical 'sense-cognize-act' model: the computer program uses an internal data structure to represent the board and, after taking in the starting position, uses that internal representation to solve the puzzle as a whole, and finally, when all the 'cognition' has been done, writes down all the numbers. 
However, this is clearly not how we humans do this. We work with the Sudoku board in order to solve the puzzle: we look at what numbers we have in the grid, figure out a new number, write down that number in the grid, and proceed. So, again, the explanation of how we solve a Sudoku puzzle is likely one that includes many perception-process-action cycles.
Another common strategy to solve Sudoku puzzles is to write down, in small font, which numbers are possible in a cell. Or, when one is 'stuck', one guesses a number to proceed, but uses a pencil to indicate the guesses from the certain moves. Such 'annotations' serve as external markers that become part of the cognitive process of solving Sudoku puzzles. 
There is also the interesting phenomenon of what the cognitive scientist David Kirsh calls 'projection'.Projection is where we don't explicitly write down some guess, but imagine having written down the guess while at the same time looking at the Sudoku board, and proceed accordingly. Projection is thus a kind of Augmented Reality where we combine external representations (the Sudoku board and the numbers that are actually on it), together with internal representations (neural codes) to form the percept that we are working with. 
In all these instances, it should be clear that we are not solving problems with our brain alone. Our cognitive powers and abilities are derived from complex interactions between our brain, our perception-action system, and parts of the environment. 

There is a lot to be said for the view of Embedded Cognition from an evolutionary point of view. Presumably, much of what the brain has done for millions of years is control. Elements of higher cognition that we more naturally associate with 'intelligence', like reasoning, decision-making, or planning, came much later on the scene. As such, it would make that we developed these higher-order aspects of cognition on the basis of the kinds of more elementary processes involving perception and action. 
Another argument for Embedded Cognition is that it would seem to be the only reasonable explanation as to why it is that modern humans seem so much more 'intellectually advanced' than our predecessors from, say, several tens of thousands of years ago, even as our brains are not be all that different. Indeed, Alfred Wallace, who put forward ideas of natural selection and biological evolution around the same time as Darwin did, found himself unable to explain the cognitive differences between humans and our primate ancestors in terms of our biological differences, and proposed that the explanation had to lie elsewhere. 
Embedded Cognition provides exactly such an alternative explanation: while our brains may be similar to the brains of our ancestors, we have the collected works of millions of thinkers, while our ancestors did not. Indeed, language allows for cultural transmission: thoughts, concepts, ideas, beliefs, tools, skills, behaviors, habits, procedures, and routines can all be learned, copied, tweaked, and modified by society at large, and across large distances in both space and time. Thus, cognitive development is not so much a function of biological evolution, as it is of cultural evolution.
As a concrete example to illustrate this idea, think of the phenomenon of co-discovery, where different people independently come up with some idea, theory, or invention. If one believes that these are products of the brain alone, then the chances that two people come up with the same idea at the same time, but completely independent from each other, should be very small, and co-discoveries should be very rare and highly unusual events. However, co-discoveries are commonplace. 
Some popular well-known examples are Leibniz and Newton both 'inventing' calculus and, ironically, Darwin and Wallace both coming up with the idea of biological evolution driven by natural selection. Again, at first sight these seem highly unlikely co-occurrences, but on the view of embedded cognition, we should not be surprised at these kinds of co-occurrences at all:
First of all, note that none of these thinkers, intelligent as they were, came up with these ideas completely out of nowhere: the basic ideas underlying calculus go back at least two thousand years ago, and at the time of Darwin and Wallace, evolution through artificial selection was a well-known phenomenon. In other words, what these thinkers did, was really to modify and tweak already existing ideas. 
 Second, and most importantly, these already existing ideas existed in the public 'sociosphere': both Newton and Leibniz were building on the work of others, so these were ideas that were available to anyone who bothered (and had the means!) to read about them. Indeed, scientists can often feel that new theories are 'in the air'. 

OK, so our cognitive abilities may be helped by interactions with the environment. However, we can cognize without interacting with our environment as well. Indeed, on the view of embedded cognition, how is it that I can sit in my chair and think about something that I am not directly engaged with? 
Well, go back to the example of catching a baseball. If I am someone who frequently catches a baseball, then the brain will start to be able to anticipate what now sensory input you would get on the basis of current sensory input and what motor actions I take. Indeed, the brain can start to create 'short-cuts' between its action centers and its perception centers. That is, normally the neural signals in the motor control centers of our brain lead to muscle contractions and other physical movements, causing changes in the world and our relative presence therein, and thus to changes to the sensory input we will get next. However, the brain has learned to anticipate what the sensory input should look like as the result of that action. As such, part of the brain has taken on the 'role' of the external world, and is used as a simulation of the world, while previously existing parts of the brain still control the perception-action cycles to interact with this simulated world, just as it would as when it would interact with the actual world. As such, I can 'play out' the act of catching a baseball in my head.
So, we can indeed cognize about the world without having to interact with the world, and we use an internal representation of the world to do so. However, the explanation of my cognitive abilities is still one that is inherently a situated cognition one; it is best understood as the 'core' brain interacting with an internalized, simulated environment, much of which involves controlled cycles of perception and action. 
One of the most powerful 'props' that humans have learned to use to augment their cognitive powers with is language: whether it be natural language, specialized languages such as used in mathematics or the sciences, or any other system of external markers. This, of course, should not be surprising from a computationalist point of view: it is language and other symbolic representations that allows one to process information. Indeed, by using representations, one can obtain cognitive powers that without the language one may not have. For example, if it wasn't for the paper and pencil in our environment, we would be hard-pressed to figure out the product of these two numbers. Thus, while I may already be a cognitive agent due to the representations in my brain, by adding external languages such as English and mathematics, I can further enhance my cognitive abilities. 
Using an external natural language is a very late development in the evolution of cognition. Most cognitive beings do no such things, and from an evolutionary point of view, we humans have only done so for a very short time. From that perspective, to suppose that the internal representational systems inside a cognitive agent should mirror the representational symbol systems of natural language, is quite unrealistic. Indeed, it is likely that it is the structural, combinatorial, and recursive nature of our natural languages that increased our cognitive powers, meaning that our already existing internal representations were of a different nature.
This last observation is especially important for when we discuss AI architectures in Part II of the book. Many AI architectures suppose that the representational systems that underlie cognition have the kinds of structural and recursive features we find in our natural and formal languages. And indeed, such structured representations may well be needed in order to attain higher and explain higher-order cognitive abilities such as deliberate reasoning and decision-making. However, these symbol systems also sit on top of a more basic computational and cognitive system: this is the system that helped create those more structured representations in the first place, the system to which the structured representations are meaningful, and the system which constantly modifies the structured representations for uses it sees fit. In other words, there is a system that 'breathes life' into the structured representations. A system of structured representations alone is merely a tool.

When an already existing cognitive system creates a new symbol system to expand its cognitive powers, it might actually help our understanding of the situation to think of two cognitive systems: one cognitive system (cognitive system A) interacts with symbols to create a second cognitive system (cognitive system B). The symbols are external to cognitive system A, but internal to cognitive system B. 
This seems a weird way to think about humans. Don't humans have just one mind? Shouldn't we be associating one cognitive system per one body? Well, go back to the notion of a universal machine, such as your laptop. Without any software, your laptop clearly lacks abilities that with a program it does have. That is, while physically your laptop still looks the same as before, its information-processing abilities have changed a good bit. And so it is, according to Embedded Cognition, with humans. That is, our physical or biological being is not the same as our cognitive being. And, as far as cognition goes, there are times where it makes sense to associate multiple cognitive beings with a single body, just as we can associate two sets of different computational powers to a laptop that can run other programs.
Go back to the example of making a note to oneself as a reminder of, say, some upcoming appointment. Now, first of all, should we consider the note not to be part of your memory just because it isn't part of your body, let alone reside in your skull? Sure, the note is not part of you as a biological entity, but when it comes to you as a cognitive entity, it sure would be helpful if at some later point we could simply say that you remembered to come to the appointment. But to do so, we need to make making the note part of the cognitive system that had the memory regarding the appointment. Now, of course, it is quite possible that you never saw the note again and, as such, forgot about the appointment. In that case it makes sense that the note is not part of the cognitive system that we point to. In short, there are two ways to parse the situation into cognitive systems, and which cognitive system we should talk about all depends on what we are trying to explain and predict. Indeed, this is not unlike a laptop, where depending on what we are trying to explain, we sometimes refer to the operating system running the machine, and at other times refer to the program executed by that operating system.

Note that we can simulate the use of an external symbol system in our head. For example, when we sit in a chair and think about a problem, we often speak with an 'inside voice'. Now, first of all, why do we speak to ourselves at all, whether out loud or with our 'inner' voice? This seems strange: don't we know what we're thinking? Why would I have to tell myself what to think? But, on the Embedded view of Cognition, the phenomenon of talking to oneself would have the following plausible explanation. 
As pointed out before, our cognitive powers are greatly affected by living inside a society. That is, by talking to other people, we can extend our cognitive capabilities, as other people may have knowledge or viewpoints that are different from ours. However, such conversations and dialogues are inherently back-and-forth, and thus iterative: no one has already figured out their thoughts and ideas at the start of the conversation, and it is by going through the very process of making points and counterpoints, and of successively going through modifications, that we end up where we do. As such, we figured out that we can be an effectively partner in dialogue with ourselves: we express an idea and, upon hearing it, provide criticism and elaboration. 
The same thing happens when we write a paper: we express some ideas on paper, read back what we wrote to see if it makes sense, make changes if needed, and go through many of these cycles. So, talking to oneself does serve a practical purpose, as it allows us to have a constructive dialogue with ourselves. And if we now simulate such a conversation with ourselves inside our own brain, we end up with an inner voice that allows us to have that dialogue with ourselves inside our head, and think while overtly sitting silently in a chair.
Figure 4.1 shows some of the ways in which cognitive systems can be embedded in an environment, and how we can use language and representations to extend our cognitive powers.

Some critics of Embedded Cognition have likened it to a kind of Behaviorism. However, Embedded Cognition really doesn't subscribe to Behaviorism at all. Sure, Embedded Cognition often stresses the importance of behavior as far as cognition is concerned. However, as some of the examples should have made clear, Embedded Cognition states that there is plenty of cognition that can take place without the presence of overt behavior. Most importantly, even if behavior is involved, that behavior is only part of a larger functional system to which the cognitive vocabulary can be attached. And, this larger functional system will certainly include the functionality of the brain! So, Embedded Cognition rejects Behaviorism.
On the other hand, Embedded Cognition is compatible with Functionalism. That is, while in our earlier discussions we have talked about Functionalism in terms of the functionality of the brain (we might call this Brain Functionalism), Embedded Cognition states that the relevant functional system may, at times, include more than the brain and, at other times, the functionality of one part of the brain relative to another. As such, the view of Functionalism can be broadened so as to encompass functional systems other than just the brain as a whole. In fact, the ideas of Embedded Cognition go hand in hand with the very idea of Functionalism that the relevant functionalities are multiply realizable: for example, whether we implement memory through a neural mechanism, or through a notebook, makes no difference as far as having that functionality goes.
Likewise, the views of computationalism and embedded cognition are quite compatible. Think, for example, of how we figure out the product of two numbers. Rather than trying to figure this out in our head, we go through a sequence of interactions with our environment where, depending on what we see in front of us, we perform a small action. Indeed, especially when it comes to external symbol systems, like natural language, mathematics, diagrams, etc. the account of mind and cognition follows a computational as well as embedded cognition point of view: relevant computations to which the mental phenomena reduce are implemented by parts of our brain, body, and environment.
On a final note, remember that in section~\ref{hardwareandsoftware} we pointed out that our brains are unlikely to be Universal machines. Humans, however, clearly are universal machines: you can give us any description of an effective method, and we can, by the very definition of it being effective, follow that method. The difference is of course that humans have a sensory-motor system: eyeballs and hands that brains, by themselves, do not. That said, it is of course possible to simulate the processing of such an external symbol system inside our brain. The relevance of this latter observation will become clear at the end of the next chapter.

Our detailed discussion on the nature of computation also allows us to more carefully state the thesis of computationalism. First of all, when we cognize, we cognize something. So, as a theory on cognition, computationalism clearly needs to appeal to semantic computations, rather than mere syntactic computations. And indeed, this is the more carefully phrased theory of computationalism: cognition reduces to computation.

The claim that cognition reduces to semantic computation seems to immediately present a problem, however. In the example of a Turing-machine computing the addition function, the meaning was made on how we, as human users of the computation, interpret the symbols. That is, representations become meaningful thanks to a human cognizer. However, that is not an option for computationalism: we cannot rely on human cognition in order to explain human cognition! So, how can a computation become a semantic computation, and thus be about something, without?
Indeed, a calculator by itself is merely a system of syntactic computation. If there is any meaning attached to the numbers, it is because we, the human users of the calculator, assign meaning to those symbols. Indeed, the calculator by itself doesn't provide any meaning to the symbols, let alone that it would somehow 'understand' this meaning. As we will see, this is in fact the essence of the objection levelled against computationalism by the Chinese Room argument, which will be discussed in Chapter 5.
Indeed, the issue of how representations can come to be meaningful is a deep philosophical issue, and it is in fact the topic of the Third AI War. So, much of the discussion will have to await that section. However, in order to avoid computationalism to be rejected immediately at this point, let us at least point to a way in which representations can come to be meaningful without human cognizers providing that meaning.
Suppose that we equip the calculator with a sensory-motor system that allows the calculator to interact with its environment. Indeed, let's suppose that certain internal symbols are being 'activated' in systematic response to what such a 'robotic calculator' encounters in its environment. Then we could say that it is of the causal connections between the robot's internal states and the outside world that the internal states come to represent what they do. For example, if the same symbol '4' is used in response to seeing four cookies or four dogs, then it is reasonable say that this symbol is used to represent the number $4$. In general, then, the idea here is that representations represent things because of the causal role they play in relation to the larger computational system, and how that system causally relates to its environment.
Indeed, in the case of humans themselves, a computationalist would point to the ability of the human to sense and act in the external world that makes the 'neural codes' in our brains mean what they do. In fact, sensing could be seen as the very 'encoding' function that humans employ to create the neural representations that our on-board computer, the brain, can work with. Likewise, given how that neural activity eventually leads to the activation of our motor control functions can be seen as a kind of 'decoding' of those neural activations into something meaningful.
Notice that on this view, limitations on our sensory-motor system will limit as to what is semantically computable or not, and thus what we are cognitively able of doing. Limitations to our perceptual resolution, for example, will automatically limit what our minds can figure out regarding whatever presents itself in the world around us at some specific moment in time. In fact, if the Physical Church-Thesis is true, then any transformation between any object and its representation as performed by the sensory-motor system will fall squarely within the Turing-limit, and hence any deviant codings are simply not possible.
All of this, of course, is a very crude and simplistic view on the role of our brain and how its neural firings come to represent and compute things. Indeed, having some symbolic activity in response to seeing a dog is one thing, but what about our thoughts of abstract or imaginary objects? What causal role to the 'larger' system would representations of such objects have that would make them represent what they do, when there may be nothing in the external world corresponding to them? So, exactly how representations come to be meaningful is actually not an easy question. Again, we'll have a whole section of the book devoted to this issue.
For now, however, let us address another complaint one may have regarding this view. The complaint is that on this view, explanations of mind, cognition, and intelligence seem to go beyond mere symbol manipulation. Indeed, if we add sensory-motor systems, aren't we going beyond computations? So, how is this still a computationalist view? 
Again, it needs to be pointed out that the view of computationalim is that cognition reduces to semantic computations, not syntactic computations alone. That is, cognition is held to reduce to embedded symbol systems. In fact, in doing so, we see how computationalism holds on to the second important dogma of functionalism as well: the of the syntactic computations in its larger context matters. Indeed, computationalism is a variant of functionalism, claiming that the relevant kind of functionaltity relevant to coognition is of a computational variety.
Finally, we should note that on this view, input and output transducers become part of a that is more than just the syntactic computations alone. This is an important point. The original conception of effective computations was imagined as something akin of a human performing long division. As such, the symbols are to the human, and also meaningful the human. However, the symbols used by these larger computational systems have become symbols. And, while meaningful in virtue of the causal, functional role they play within the larger system amnd how that system interacts with its environment, the symbols are no longer meaningful to the computational system as a whole. This, the computationalist claims, is indeed the situation with the human brain: the human brains contains representations of the world, but unlike external symbol systems lkike natural language and mathematics, these internal representations are not at all meaningful to us.

Sometimes, computationalism is described as saying that the mind is to the brain as what software is to hardware. While this analogy captures the idea that the mind is not a physical object, but rather a kind of abstraction, the analogy is otherwise actually quite problematic, and almost definitely plain wrong. 
To see this, notice that the distinction between hardware and software really only applies to universal machines: the program that we give a universal machine is software, and the universal machine itself is hardware. Now, since most computers around us are universal machine, we tend to think that all computers are like that: pieces of hardware that can 'run' some piece of software. However, it is important to remember that Turing-machines need not be universal machines at all. Such machine would be pieces of hardware, without running any kind of software: rather than following an algorithm that is given to them as an explicit set of instructions (the program), such a machine is hardwired to obey a certain kind of algorithm.
Now, it seems highly unlikely that our brain is a universal machine. That is, it is highly unlikely that there is some kind of central processing unit inside our brain that 'executes' an explicit set of instructions, stored elsewhere in our brain. Indeed, science fiction movies that play with the idea that the mind can somehow be 'downloaded' on a memory stick and that, as such, can be 'copied', modified, or 'uploaded' back to some human brain, make very, very little sense. In fact, even if there was some set of instructions guiding the working of the brain, then this still would not be 'the mind', since the mind is seen as the abstract functionalities that exist courtesy of the brain. Again, as Turing has reminded us several times now: the level of 'mind' and 'cognition' is likely to be found at a 'higher' level than the level of individual neuronal firings. 

Most computationalists regard it implausible that the brain would be able to exploit 'the infinite', which is why they typically claim that the computations underlying cognition are within the Turing-limit. Thus, if computationalism is correct and if one's cognitive capabilities are the result of computational processes, then our information-processing powers are bound by the Turing-limit as well. 
Notice that this becomes a non-trivial and thus potentially testable prediction. While Turing showed us limits to effective methods of symbol manipulation, the human mind as a whole of course need not be constrained by this. That is, maybe the human mind has some kind of mental capability to figure things out without systematic processes of symbol manipulation. 
So, do we maybe have evidence that the human mind can indeed go beyond the Turing-Limit? It is hard to actually prove this, since any finite amount of claims made by humans can, at least in theory, always be the result of a computation within the Turing-limit. Also, processes of symbol manipulation are often exactly how we create proofs, and verify them. If the human mind figures something out without an accompanying proof, then ow can we be sure that what the human mind put firth is indeed correct? 
Still, we might be able to provide evidence that the human mind is more powerful than Turing-computation if the humans mind do things that to indeed be correct. For example, if humans steadily give the right answer to the halting behavior of a machine, or solve pother kinds of question shown to be beyond the Turing-limit, then at least we would have evidence that the human mind is more powerful.
However, there does not seem to be any evidence in this regard. Like computers, humans don’t seem to be able to solve the halting problem either. In fact, humans are stumped by the halting behavior of the very same machines that computers are stumped by. Also, problems that computer scientists deem ‘hard’ (e.g. whose computational run-time increases exponentially as a function of the number of variables on the input), humans likewise deem to be hard. Now, none of this should be a surprise if humans use external symbols as tools to represent and solve the problems, and humans often do exactly that. However, if the human mind had some other, non-computational, way of solving these problems, then one might expect humans to possibly be categorically faster. But this does not seem to be the case. In sum, computationalism is testable and falsifiable, and the fact that humans seem to run into the same barriers that Turing-computers run into is evidence in favor of the thesis of computationalism. 
In 1980, John Searle published one of the most widely discussed arguments that purports to show that intentionality cannot arise from computations alone: The Chinese Room Argument. Now, before going into his argument, let us say a few words on the notion of intentionality, and why this is important for our discussion.

As we noted in the previous chapter, when we humans think, we think about things, since this is what thinking is for. We think: “What's for lunch?,” “Whom should I vote for?,” “Jones is tall,” “Is it just that the wicked flourish?,” “It’s cold on the top of Mount Everest.” These are garden-variety thoughts about garden-variety things: lunch, voting, Jones and being tall, the wicked and flourishing, and the temperature on the summit of Mt. Everest. Also as noted, all the items in this list are the contents of the associated thoughts. Arguably all thinking in all thinking beings is the manipulation of such contentful thoughts (Markman and Dietrich, 2000). 
The same can be said for all mental or cognitive activities: a memory is a memory of something; a perception is a perception of something; a decision is a decision about something. The technical term for this kind of aboutness is intentionality. That is, our mental states and cognitive activity is intentional or meaningful to us.
Earlier we discussed the case of the simple calculator, and we declared that while the numbers that the calculator are meaningful to us, the human user of that calculator, presumably they are not meaningful to the calculator itself. Hence, the calculator lacks intentionality: it doesn't have thoughts. In general, regardless of the difficulties discussed in the earlier sections to define 'intelligence', what it means for some agent to be 'cognitive', or what it takes to 'have a mind', if whatever is going on with a computational system is not intentional to that system, then it seems reasonable to declare that the system cannot think, and does not have a mind.
So: Can computers have contentful thoughts? If not, then it seems reasonable to declare that computationalism is false. We could engineer fancier and more powerful calculators, but the central, long-dreamed-of goal of AI, would be dead in the water.

Imagine you are locked in a room. With you in the room are a large book and all the necessities you’ll need to be healthy and moderately happy. Strings of symbols written on pieces of paper are slid through a slot in the door. Your job is to look up the strings symbol by symbol in your book, write down on paper other similar symbols per the instructions in the book (which, we can suppose, is written in English), and then slide your pieces of paper back out the slot. 
Unbeknownst to you, the symbols are Chinese characters (there is no one language called Chinese; this fact will be ignored). We assume that you know no Chinese at all. (If you do, substitute some other language of which you are completely ignorant with an alphabet different from any language you know.) You go about your job, laboriously reading and following the instructions in the book, writing Chinese characters, to the best of your ability, on sheets of paper, and sticking the sheets out through the slot. This goes on for months, during which time you are otherwise well cared for. 
Also, unbeknownst to you, outside the room are several Chinese scholars discussing, say, Chinese history. Because of the insightfulness and erudition of the comments coming out through the slot, they believe that the person in the room is an expert on China's history. You are not an expert on Chinese history; worse, you are completely ignorant of what language you are working on. Indeed, you entertain the idea that the symbols you are manipulating are not in a language at all. The only reason you think they might be is that the book of instructions seems so complete and organized, even if ponderous. So, outside the room, an erudite discussion of Chinese history, instead the room, a tedious manipulation of meaningless symbols. 
Here is where Searle springs his trap: you in the room with your book and sheets of paper are computing. In all relevant respects, you are implementing a computer program – a program for communicating in Chinese about Chinese history. In fact, the large book in your room contains the algorithm (or implements it in English). You are simply following the algorithm (again, unbeknownst to you). Since you are computing (i.e., following an algorithm) and since you have no idea what you are “talking” about (Chinese history), it follows that neither would a real electronic, digital computer, implementing the same algorithm you are implementing by reading the large book. 
Since there is nothing special about Chinese history, the argument metastasizes. The essence of computing, so Searle’s argument says, is doing exactly what you are doing: algorithmically manipulating mere “marks” without regard to their meaning. Hence, no computing is processing contentful symbols. Computing is contentless manipulation of marks (in a computer, these marks are electrical signals). But we noted above that thinking is processing content – all your thoughts have content, and the content they have is crucial to the way they are manipulated and processed. Ergo, computers can’t think. Since thinking is a necessary condition for intelligence, computers can’t be intelligent: the AI project is hopeless. 

The replies to this argument would fill a library of many rooms. AI-friendly philosophers and AI researchers labored to refute it. Searle claims to have handled all alleged refutations (see his 1980, 1990, 1992). Here are some of the better-known replies, already discussed in Searle's original paper:
The System Reply
The System Reply states that while the Chinese symbols are meaningless to the man in the room, the man is only part of a larger system (including the rulebook, pencils, paper, and possibly other things), and it is possible that that larger system has contentful thoughts
The Robot Reply
The Robot Reply suggests making the room into the head of robot (a large robot), by attaching cameras and microphones for vision and hearing., and actuators to manipulate its environment, and having this robot interact with the group of Chinese historians. The Robot Reply is thus a specific instance of the System Reply, in that the thought is maybe the larger system (the robot), of which the man is merely a part, has contentful thoughts
The Brain Simulator Reply
The Brain Simulator Reply suggests that at least in principle, the rules in the rulebook could form a simulation of what is going on inside the brain of a person who does speak Chinese, and whose understanding and intentionality with regard to those symbols presumably come from his brain. As such, the Brain Simulator Reply is once again a kind of System Reply: this time, the system of man and rulebook implement the working of a brain, thus potentially giving rise to the relevant intentionality that Searle claims is missing.
It is our belief that the Chinese Room argument is invalid. In the next section, we will carefully analyze the Chinese Room argument, and identify its pitfalls given our earlier discussion of the computations. During this analysis, the System, Robot, and Brain Simulator Reply will make a reappearance and, contextualized by our analysis, their particular strengths and weaknesses will become more clear. In the end, we believe that some form of the System Reply is indeed the correct reply.

Searle's Chinese Room argument is basically this:
Premise Premise 2. The man in the Chinese Room does not understand what the symbols mean that he is manipulating, and thus lacks intentionality
Premise Conclusion: Computations cannot lead to intelligence
How good is the argument? In the following sections, will find three kinds of complaints come over and over: 
The Theoretical Complaint
If Searle's argument is to work against computationalism, then we need to make sure that the Chinese Room scenario is able to cover any kind of computation. It is here, we believe, that many accounts of the Chinese Room argument are not always too careful. That is, depending on the details of the set-up, the argument may not generalize to all possible computations.
The Practical Complaint
In fact, we will argue how many accounts of the Chinese Room scenario (Searle's own included) do not even include the kinds of details that are arguably relevant in order for the Chinese Room set-up to even be successful in 'pulling off' the feat of holding up its end of having a rational and insightful conversation, in Chinese, about Chinese history (from now one, we will refer to this feat simply as 'the feat' that is being 'pulled off'.) 
The Rhetorical Complaint
Finally, and maybe most importantly, we argue that if all the details are provided in order to pull off the feat, quite a different picture of the situation will emerge as opposed to the one suggested by the typical accounts. Typical accounts tell the 'story' of the Chinese Room in such a way that many readers and listeners will indeed be persuaded that there is no intentionality of the relevant kind. However, adding the necessary details of what it would take to pull off the feat paints quite a different picture; one in which it is far less clear that there is no intentionality of the right kind. 
In sum, we argue that typical accounts of the Chinese Room argument are a straw man and are misleading the reader. The typical presentation of the Chinese Room argument is seducing the reader to think certain things: it pumps certain of the reader's intuitions, but hides details that may pump quite different intuitions. On to the details. Or: let's 'refurbish' the Chinese Room! 
In his original argument, Searle did not spend much time on describing the nature of the instructions other than to say that the instructions were for manipulating Chinese symbols strings. This is unfortunate, since some subsequent presentations of Searle's Chinese Room argument in the literature and particularly online suggest that the instructions work like a very simple 'lookup' table. That is, according to some accounts of the argument, the instructions are really nothing more than a long list of 'input-output' pairs, that effectively say: "If you get X on the input, then provide Y on the output". However, there are serious problems with thinking about Searle's room in this manner:
Look-up Tables don't cover the whole space of computations
The Theoretical complaint against look-up tables is that lookup table algorithms are a very small (infinitely small, in fact) subset of all possible algorithms. However, if Searle's argument is an argument against computationalism in general, then no restrictions should be placed on the nature of those algorithms. As a proper argument against computationalism, it should be made clear that the rule book may force the man in the room to take many steps of symbol-transformation, not just one.
Lookup Tables are Unrealistic
The Practical Complaint it is utterly unrealistic that a look-up table would be able to produce the kind of input-output functionality that would be required for 'the feat'. Think, for example, of all the possible ways the people on the outside can phrase a simple thought, idea, or question. We can use different words, and change their order. Also, due to false starts, many sentences that people utter in real life conversations aren't even grammatical. In fact, every human being of 'normal' intelligence can correctly answer an infinite number of questions about the addition of numbers. A lookup table, which has to be of a finite nature, cannot do that; but multi-step algorithms can. And sure, while any conversation is of finite length, and thus there is always a possibility that a lookup table could pull off the feat, the chances of that happening are quickly seen to be infinitesimal once you contemplate the enormity of the space of possible conversations.
Look-up Tables trivialize Computations
Finally, the Rhetorical Complaint is that in characterizing the computations as mere look-up tables, one 'trivializes' the nature of these computations. Indeed, as we saw Turing point out earlier, when people see a simple mechanism, they are immediately less likely to attribute 'intelligence' to it than when the mechanism is more complicated. Thus, a proper account of the Chinese Room should acknowledge the possible complexity (that can go far beyond mere look-up tables) of the computations we might be dealing with.
Notice that it is the purely theoretical aspect that should be the immediate death knell of any Chinese Room argument that presents the rulebook as a mere look-up table, for look-up tables do not cover the whole space of computation, and hence the argument fails to generalize to all possible computations, and thus fails as an argument against computationalism. However, without addressing the rhetorical ‘trickery’ involved in the accounts, many readers will have a hard time grasping this fact. Fortunately, by focusing on the practical aspects of the thought scenario, and highlighting the kinds of details that need to be taken care of in order for this argument to really work, we believe that we can steer the reader back into the right direction, or at least away from the all too seductive conclusion Searle wants us to draw.
So, let's add those details in light of what we just discussed. That is, let's adjust (refurbish!) the room so the man is able to perform multi-step computations, which avoids the completely unrealistic expectation that somehow a look-up table would be sufficient to hold up an intelligent conversation. Now, this should not be hard: we can simply provide the man with plenty of scrap paper that serves as a kind of 'working memory' in order to perform the gradual transformations as per the instructions in the rulebook. That is, the rulebook, like a Turing-machine program, may specify algorithms that take multiple steps for the man to transform input string into output string, but the man has the scrap paper, which of course works just like the Turing-machine tape, to make the gradual syntactic transformations as specified by the rulebook. Thus, adding scrap paper will ensure that the scenario can cover computations of any kind, which is necessary for the argument to be a proper argument against computationalism. 
OK, that's a good first step, but of course this will leave Searle completely unimpressed: "Sure," Searle will say, "the transformations may take multiple, if not many steps. So what? The man still doesn't understand what the symbols stand for, and so the relevant intentionality is still lacking." However, the problem of using look-up tables (and the three ways in which this plays out) turns out to be just one instance of a more general problem with Searle's argument by not being very careful about the nature of computation. There is more refurbishing to be done! 
While some presentations of the Chinese Room argument gloss over the fact that the transformations that the man has to perform can involve multiple steps, even more presentations do not include anything that would make some kind of long-term memory capacity possible. That is, many presentations of the Chinese Room argument assume that the man in the room provides its output merely on the basis of input and the rulebook, and that is yet again a problem from each of the three aforementioned perspectives. 
First, if Searle's argument is to refute computationalism, then the argument should include the capacity for long-term memory which, as we already argued in section..., can and should be integrated into any kind of computational account of cognition, mind, and intelligence. Indeed, without long-term memory, any kind of learning becomes impossible. So, from a purely theoretical perspective, the lack of some kind of long-term memory in the Chinese Room set-up is unable to cover the kinds of computational systems that computationalism claims are necessary and sufficient for cognition.
Again, though, this point is probably better appreciated from a practical point of view. For example, think of how the whole set-up is supposed to uphold some kind of intelligent, rational conversation. Well, conversations typically follow some kind of trajectory where one often refers back to things already said. That is, the history of the conversation provides a context in which to appropriately process whichever input comes in next. 
For example, if in the middle of the conversation the people outside the room ask "Can you please elaborate on that?", then any kind of appropriate response will have to take into account whatever preceded this. In fact, a cursory look should reveal that this is so with almost every utterance made in conversations. Also note without any long-term memory, the person in the room will always generate the exact same output given the same input, which will quickly reveal to the people on the outside that they are not dealing with a normal, intelligent, human being. In sum, without long-term memory, the Feat simply becomes impossible.
Now, how can we adjust the scenario so as to make sure long-term memory is present? Again, this is not hard to do: we simply have to add some filing cabinets to the room, which can be used to hold papers with symbols on them. That is, the instructions in the rulebook may at times dictate that the man write down some symbols on the sheet of paper and store it in some specific place in the filing cabinet. And of course, other instructions may ask the man to retrieve pieces of symbols from the filing cabinet and, depending on what symbols are found there, continue the symbol transformations one way or the other.
By not making reference to any such filing cabinet or other method to implement some kind of long-term memory, we thus once again realize how the typical account of the Chinese Room simplifies and trivializes the complexity of the computational system involved (this is, of course, the rhetorical complaint in this context). By spelling this out more carefully, quite a different picture may emerge.
Of course, Searle will still not be impressed: "OK, fine,", Searle will say, "we add filing cabinets. Big deal! Does this help the man understand the symbols? Clearly not! So, my argument still stands." Unfortunately for Searle, we are far from done.
The symbols that are being manipulated by the man in the Room are Chinese symbols. Initially, this would seem to make sense, since the conversation is in Chinese as well. However, are we to assume that a system's ability to converse in Chinese has to have Chinese symbols for its internal representations? Can no other kinds of symbols be used? Again, let's think about this from our different perspectives. 
This time, let’s start from a practical perspective. For example, consider that the conversation about Chinese history turns to some war that the Chinese were fighting, and the tactics and strategies that may have been involved from geographical and temporal points of view. Just naively, it would seem that it would be very hard to see how merely using Chinese symbols would be sufficient to, for example, consider the movement of some Chinese army or fleet. Indeed, in general, it is not unlikely that the Chinese people on the outside would like to be able to draw a little diagram, and pass that to the man inside the room, just as we not infrequently use diagrams, in addition to just words, during some conversation or discussion. 
Of course, Searle might object to the use of diagrams in the scenario, as seeing some kind of schematic or diagram will certainly be a great tip-off to the man inside what the conversation might be about. However, the thought experiment can be easily adjusted to fit Searle's needs: if any visual diagrams are used in the conversation, then the diagrams will be transformed into a long string of symbols, not unlike how a picture is stored on a digital computer as one long series of 1's and 0's. Presumably, Searle will have no objections to doing this, as the argument can still go through as before: the man in the room will have no idea what these symbols stand for or represent, and thus lacks the relevant intentionality.
In fact, the Robot Reply will extend this line of thinking. Of course, the Robot Reply says, images will be transformed: this is exactly what sensors do. Specifically, according to the Robot Reply, sensors like cameras and microphones will transduce images and sounds to internal representations. And, on the output side, if we were to draw a picture in response, we need actuators that translate the output string, as generated by the man in the room, into actions (the physical drawing of a picture, for example) in the environment. These would seem to be pretty useful, if not necessary, in order to Pull off the Feat. 
OK, so far so good. But here is the important point the Robot Reply makes: even the Chinese expressions written on paper used by the people on the outside will be transduced. Indeed, words on paper are not any different from any other visual input that cognitive systems have to deal with, and the internal representations that these visual inputs get transduced to will not look anything like Chinese symbols. Likewise, the commands that get sent to the actuators to draw a picture or grab an object will be nothing like 'draw a house' or 'grab the apple'.
Similarly, the Brain Simulator Reply says that when a Chinese speaking person hears or reads expressions of the Chinese language, their brain isn't directly working with words, but with visual and auditory stimuli, that the eyes and ears translate into neurological activations. And again, those neural representations are not anything like Chinese symbol systems.
Indeed, as we will see in part II of the book on architectures of the mind, it is only the classic symbol system architecture that might propose something along these lines of "Cognizing about X requires the manipulation of a symbol 'X'". However, computationalism is not committed to that particular architecture. Connectionists, for example, believe that the internal symbol system implemented by human brains is completely unlike the Chinese symbol system. 
In other words, we should be careful to separate between internal and external representations. Chinese characters are part of a symbol system that is, like any other natural language symbol system, external to human beings. But why should we believe that the internal symbol system, as postulated by computationalism to underlie the use and understanding of such an external symbol system, be anything like that external symbol system?
Of course, talking theoretically, since all computation can be done using just 0's and 1's, just two Chinese symbols should be sufficient to Pull off the Feat, assuming the truth of computationalism. However, in the use of two Chinese symbols as 0 and 1, their meaning would no longer be the meaning as when they are used in Chinese conversation. Indeed, as discussed in section ..., a symbol is what it is not because of its physical implementation (e.g. its physical shape and form when written on a piece of paper), but by the role it plays in the larger symbol system. A Turing-machine that were to write one of two Chinese symbols on its physical tape isn't using a Chinese symbol system ... it is a binary system, plain and simple. To emphasize the difference, such a Turing-machine could be said to be using Chinese characters, but not Chinese symbols. 
From a purely theoretical perspective, then, using a Chinese symbol system unnecessarily restricts the nature of the computational system. As such, the Chinese Room argument does not cover the whole space of computation, and falls short as an argument against computationalism.
As before, though, the real strength of this objection is best seen from the rhetorical point of view. This is because by assuming that all that is being manipulated inside the room is Chinese symbols, Searle creates the suggestion that the relevant intentionality towards the Chinese symbols used in the conversation needs to be towards the symbols inside the room as well. Moreover, the obvious candidate to have such intentionality towards the symbols inside the room is of course the man who is also inside the room. But since the man clearly lacks any such intentionality, it is highly seductive to draw the conclusion that the relevant intentionality is not present at all. 
However, all of this is one big rhetorical red herring, because none of this is relevant. What is relevant, is whether the computational system, as implemented by the Chinese Room scenario, is intentional with regard to the Chinese symbols outside the room. That is a completely different question, and one Searle does not at all address. This objection to Searle’s argument is the System Reply, and it is here that we seem to be getting to the core of the matter.
There are a few other ways in which the Chinese Room argument is distracting, or at least misleading. For example, just think of what the size of the rulebook would need to be in order for the set-up to accomplish the Feat. Even if the book is not a look-up table, but instead describes multi-step algorithms, the vast space of possible conversations means that this 'book' would be really, really big! It would certainly be bigger than the size of any normal book. Indeed, it would be much more realistic that the man in the room has a whole library of books available that would have to be consulted. 
Moreover, the man would have to work inhumanly fast to get all the symbol transformations accomplished, but also this aspect of the Chinese Room argument gets downplayed. Why is this important? It is because if the Chinese Room argument was presented as the whirlwind of activity that it must be, then it becomes more clear that focusing on individual activities the man performs may well be the wrong level at which to contemplate what is going on in the scenario.
This is also a good point to note how it is actually quite peculiar for Searle to attack computationalism by imagining a scenario where the computational system has to perform the cognitive act of conversing using a natural language. Computationalism states that all forms of cognition can be instantiated by computational systems, not just the use of natural language: basic perception and action for navigating an environment, learning, reasoning, etc. Why didn't Searle imagine a scenario where a computational system is supposed to support some cognitive task that does not involve natural language?
Well, the answer should be pretty obvious: by focusing on a task that uses natural language, Searle can point to a set of symbols that are obviously meaningful to Chinese speakers, but not to the man. So, Searle can claim that something is amiss. However, had Searle discussed the cognitive activity of, say, navigating an environment, then obviously the only symbols Searle could have pointed to would have been those of an internal symbol system. And, why would those symbols have to be meaningful to any human being?
Indeed, with a task like navigation, it would have been much more clear that the man would have been a mere part of a larger system, more akin to a robot. This, of course, is part of the Robot Reply. However, there is as much reason to demand that the robot understand the symbols used in the computational system that is driving it, as our own cognitive ability to move around demands that we understand the neuronal representations our brain is working with ... which is to say: there is no such reason at all.
Of course, this does not address the issue of intentionality. That is, how is it that the internal symbol manipulations are about something?  However, this is something we already briefly addressed in the previous chapter, and something that will be addressed in much more detail in part III. For now, it is sufficient to point out that Searle has not shown that these symbols cannot be meaningful in the appropriate sense.
Finally, let us note that it is quite possible Searle was using the task of natural language since that would seem to be such an obvious case where we can talk about intentionality. That is, it is exactly when we to deliberative thinking, reasoning, planning, or decision-making, there is a clear component of intentionality: we think, reason, plan, and make decisions about something. And, as pointed out in the previous chapter, such higher-order cognitive tasks typically involve external symbol systems like natural language. So, it is quite likely that Searle wanted to know how the use of Chinese symbols can become intentional. However, if that is what Searle was really after, then it is very clear that Searle is asking about the use of an external symbol system, not an internal one. And, the computational system that would be using the external Chinese symbols is a system in which the man is merely a small moving part. Does this larger system have intentionality with regard to the Chinese symbols it is interacting with? Well, we don’t know, but it is clear that Searle has not demonstrated that the system does not have the right intentionality.
It is clear that in Searle's implementation of a computational system, the man in the room plays the role of the universal computer, and the rulebook plays the role of the program that the universal machine 'runs' by following the instructions. However, there is no requirement that a computational architecture has to be of that nature. As discussed in section ..., any computation can be directly realized and implemented as such, i.e. without any kind of explicit program at all. 
In fact, as argued in section ..., it is in fact quite unlikely that for example the human brain works like a universal machine. By presenting the computational architecture as that of being a universal machine, Searle therefore once again forces us to think about computational systems in a certain away and, as with some of the previous points, a way that is highly misleading.
To see this, notice that by using a man as a universal computer to implement the machines as described by the rulebook, Searle is able to put all the focus on the man the symbols. Not acknowledging any other necessary components of a computational system (such as long-term memory) further helps the illusion that there is nothing else but the man and the 'bits of paper': that the man inside the room is the only possible object of interest. However, we argue that pretty much the opposite is true: that the man inside the room really should not be the focus of interest. In fact, we can pretty much eliminate the man in the room.
Consider this: suppose that after being in the room for a number of years, the man gets (understandably) pretty tired of his job. So, he gets the idea of automating the whole process. He is able to get his hands on some source of energy, and rigs up a complicated system that is able to perform the necessary symbol manipulations by itself, while the man sits back and does absolutely nothing. In fact, the man can just step outside the room, and walk away.
Now, what are we to say of this? If you are Searle, you would say that there is no intentionality of the right kind: And it's not even that the man does not understand the symbols being manipulated, but that there is nothing to whom those symbols are meaningful in the first place! However, the System Reply can still say that there is an interesting system here. And, maybe that system does have intentionality of the right kind.
In the original scenario, then, the man in the room is revealed as just a 'cog' in the whole system or, as Chalmers puts it: a 'causal facilitator'. So sure, we need the man in the sense that we need some 'juice' to make the system functional and operational. But when it comes to the question as to whether the Chinese symbols used in the conversation is understood, the man's lack of understanding is completely and utterly irrelevant.
At this point, it should be abundantly clear that it is the System Reply that is fatal to Searle's argument. Indeed, the System Reply, which can be seen as a kind of general version of Robot and Brain Simulator Reply, points out that while the man in the room does not have intentionality of the right kind ... maybe the system as a whole (whether robot, brain, or other) does. Over and over, Searle does not seem to grasp the possibility that there might well be a larger system, and that this system may have intentionality of the right kind. 
Instead, Searle puts focus on the man, and we now realize that he manages to do so in several ways:
First, Searle downplays all the components that may well have to be in play in order to Pull off the Feat. However, as discussed, in addition to the man, there is a rulebook, paper and pens for performing symbol manipulations, and filing cabinets. If we take into account the complexity of the system as a whole, we might get quite a different feeling regarding the scenario.
 Second, by focusing on a cognitive task that involves the manipulation of natural language, Searle is able to point to a language that should be meaningful to the computational system, but that he claims is not. However, had Searle focused on a computational system performing some other cognitive activity, there would have been no such external symbol system to point to. And there would have been no reason why an internal symbol system should be useful to the system as a whole. 
Third, Searle choose to implement the computational system using a Universal machine setup. However, had he chosen for a more direct implementation, there wouldn’t even have been any man-inside-the-system to point to at all.
However, Searle has offered counterarguments to all replies. Let’s see what they are and how good they are.
5.Searle expresses to be completely baffled by the System Reply:

The idea is that while a person doesn't understand Chinese, somehow the conjunction of that person and bits of paper might understand Chinese. It is not easy for me to imagine how somehow who was not in the grip of an ideology would find the idea at all plausible. (emphasis by Searle)

However, Searle's counter-reply is once again highly rhetorical. 
First of all, to refer to the paper the rules in the rulebook is written on, the paper that the person in the room uses to support potentially elaborate computations, and the paper that is stored in the filing cabinet as just 'bits of paper' completely minimizes their important contributions to how the feat is being pulled off. Indeed, without any of these 'bits of paper', the person in the room surely would not be able to accomplish anything towards this. 
Second, we're not talking about a 'conjunction' of the man and 'bits of paper', but rather we are talking about a complicated functional system where the rules in the rulebook prescribe the man's actions, by reference to a filing-cabinet, and through the use of paper that serves to support potentially elaborate computations. That is, all the parts are carefully integrated and coordinated and, as such, forms a system with potential properties not had by any of its parts. 
As far as Searle's rhetoric goes, he might as well point to an airplane cut in a million pieces, and say: "well, none of those bits of metal and plastic can fly, so how can all these pieces together be something that can fly? If I take a big broom and sweep together all the bits on a big heap, clearly nothing will happen!" Searle thus makes a fallacy of composition here: just because the parts lack property P does not mean that the whole lacks property P.
Searle makes the same mistakes in his response to the Robot Reply:

Suppose that instead of the computer inside the robot, you put me inside the room and, as in the original Chinese case, you give me more Chinese symbols with more instructions in English for matching Chinese symbols to Chinese symbols and feeding back Chinese symbols to the outside. Suppose, unknown to me, some of the Chinese symbols that come to me come from a television camera attached to the robot and other Chinese symbols that I am giving out serve to make the motors inside the robot move the robot's legs or arms. It is important to emphasize that all I am doing is manipulating formal symbols: I know none of these other facts. I am receiving "information" from the robot's "perceptual" apparatus, and I am giving out "instructions" to its motor apparatus without knowing either of these facts. I am the robot's homunculus, but unlike the traditional homunculus, I don't know what's going on. I don't understand anything except the rules for symbol manipulation. Now in this case I want to say that the robot has no intentional states at all; it is simply moving about as a result of its electrical wiring and its program. And furthermore, by instantiating the program I have no intentional states of the relevant type. All I do is follow formal instructions about manipulating formal symbols.

Note how Searle pays little to no attention to the idea that it isn't the man, but possibly the robot as a whole, that has intentionality of the right kind. The only comment Searle has with regard to the robot as a whole is that the robot is 'simply moving about as a result of its wiring and its program'. But that, as we saw earlier, makes the terrible mistake of confusing micro-level properties with macro-level properties. Moreover, Searle is still looking for intentionality with regard to the symbols used inside the room/robot, but of course the only thing that we are looking for is intentionality with regard to the Chinese symbols used in the conversation between the robot and the Chinese speaking people on the outside. 
We find the same problem with Searle's response to the Brain Simulator Reply:

... imagine that instead of a mono lingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes. Now where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly doesn't understand Chinese, and neither do the water pipes ...
Correct, the man does not understand ... But who cares?! Clearly there is a larger system here, and the point of the Brain Reply is to say that maybe this larger system has the right kind of intentionality. And again, the man is unlikely to receive symbols that would any connection to how Chinese symbols are used in Chinese as a natural language: the 'language of the brain', even if it is the brain of a Chinese speaker, is likely to be quite different from the Chinese language.
We thus see that it is the System Reply that is Searle’s Achilles heel over and over. However, Searle realizes this, and has a second counter-reply to the System Reply:

My response to the systems theory is quite simple: let the individual internalize all of these elements of the system. He memorizes the rules in the ledger and the data banks of Chinese symbols, and he does all the calculations in his head. The individual then incorporates the entire system. There isn't anything at all to the system that he does not encompass. We can even get rid of the room and suppose he works outdoors. All the same, he understands nothing of the Chinese, and a fortiori neither does the system, because there isn't anything in the system that isn't in him. If he doesn't understand, then there is no way the system could understand because the system is just a part of him.

However, this reply does not work either. As explained in the section of Embedded Cognition, there really is a larger system of which the original man, as a cognitive agent (we don't care about the man as a physical or biological entity), is part. That is, part of the man's brain has been used to internalize the original environment, but together with other parts of the man's brain, that served to realize cognitive agent A, this forms a new cognitive system B. 
Indeed, the situation is not any different as with a laptop: without a certain program in its memory, my laptop does not have certain abilities. But with that program in its memory, my laptop does have that capacity. Now, did my laptop visibly change? Did we attach bells and whistles to the laptop? No, the laptop looks exactly the same as before. At first glance, nothing seems to have changed. However, because a program was installed on the laptop, its abilities have changed. 
Some commentators differentiate between 'system' and 'virtual machine' in order to further clear this up. That is, from a superficial point of view, there seems to be only one physical system, whether we are talking about a laptop, or the man. Hence, it is argued that there is indeed still only one 'system'. However, this one system instantiates two different abstract virtual machines. As such, it has been argued that the 'System Reply' is not a good reply, but the 'Virtual Machine Reply' is. In our eyes, though, the notion of 'system' is an abstract enough concept so as to capture the notion of virtual machines, and so we will simply stick to the System Reply, with this abstract understanding of 'system'.
So yes, it looks like we are only dealing with one cognitive system: the man. But, by memorizing the rule book, clearly the man has expanded its cognitive repertoire, just as a child does when learning new knowledge and skills. Indeed, when the man uses the rules to respond to some Chinese input, the man is in effect becoming a universal computer running the program as described in the rulebook, thus effectively instantiating a new system.
The Brain Reply effectively states that Searle makes the fallacy of proving too much. Indeed, imagine, as John Haugeland responded to Searle's original argument, that instead of a man in the room, we put a man inside a real brain, but that we have made the neural connections between the neurons impervious to neurotransmitters. Imagine too, that the man inside the brain gets alerted whenever some neurochemical wants to be released, and that the man has a very large supply of neurotransmitter that the man can release at these synapses so that, ultimately, the brain functionality is preserved. However, since we are talking about a real brain, then presumably all relevant intentionality should be there. So, something is wrong with Searle's argument. Put differently: Searle seems to paint himself into a Corner of Contradiction, as the logic of his argument would seem to imply that we humans cannot be intentional either.
The problem with this argument is that it assumes that the brain does give rise to intentionality. But we don’t know this. Thus, instead of claiming that in a scenario like this, intentionality will arise, the objection should merely be that intentionality may arise. That is enough to show Searle's argument to be invalid. 
Indeed, it should be noted that it is not up the System Reply, Robot Reply, Brain Simulator Reply, whatever reply to Searle's argument to argue that the system they are pointing to *does* have intentionality of the right kind. All they have to do is to find a flaw in Searle's argument that argues that the relevant intentionality does not exist, and by pointing to the man alone, Searle has indeed failed to do so.
Of course, this still leaves the question as to what intentionality is, and how a system might possibly have it. This is a question that forms the basis of the Third War on 'Mental Semantics'. For now, let us consider what Searle himself thinks about this:

It is not because I am the instantiation of a computer program that I am able to understand English and have other forms of intentionality (I am, I suppose, the instantiation of any number of computer programs), but as far as we know it is because I am a certain sort of organism with a certain biological (i.e. chemical and physical) structure, and this structure, under certain conditions, is causally capable of producing perception, action, understanding, learning, and other intentional phenomena. And part of the point of the present argument is that only something that had those causal powers could have that intentionality. Perhaps other physical and chemical processes could produce exactly these effects; perhaps, for example, Martians also have intentionality but their brains are made of different stuff. That is an empirical question, rather like the question whether photosynthesis can be done by something with a chemistry different from that of chlorophyll.
This is a very peculiar position to take, for if Searle wants to say that it is an empirical question as to whether "other physical and chemical processes could produce exactly these effects", then why is Searle so certain that in the Chinese Room these effects are not present? The Chinese Room, after all, has a certain physics and chemistry, but Searle does not run any empirical test to see if the right causal powers are present. Indeed, with this quote, Searle does seem to have painted himself into a corner.

While Searle's Chinese Room argument should be given credit for forcing computationalists to precisely lay out what it is they are claiming, the argument ultimately fails. The man inside the room does not have the right intentionality with regard to the Chinese symbols, but this is irrelevant as far as computationalism is concerned. According to computationalism, the man is merely a causal facilitator to ensure the functionality of an abstract computational system, which includes the rulebook, paper and pencil, a file cabinet, and transducers to properly interact with its environment. The Systems Reply is correct: Searle has not shown that this larger system does not have intentionality of the right kind. As Turing predicted, detractors like Searle are too focused on the 'unintelligible donkey-work', forgetting about the overall functionality of the system as a whole.

